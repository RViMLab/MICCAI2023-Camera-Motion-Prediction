{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('hil': conda)"
  },
  "interpreter": {
   "hash": "00ce21aa52e8581894864b3b01cebf6e3aad2f06f47a5e7d9b54fea21a8d1745"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/kornia/augmentation/augmentation.py:1833: DeprecationWarning: GaussianBlur is no longer maintained and will be removed from the future versions. Please use RandomGaussianBlur instead.\n  category=DeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotmap import DotMap\n",
    "\n",
    "sys.path.append('../../')\n",
    "\n",
    "from utils.io import recursive_scan2df, load_yaml\n",
    "from utils.sampling import ConsecutiveSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            file   folder\n",
       "0    split_5.mp4  video36\n",
       "1   split_35.mp4  video36\n",
       "2    split_7.mp4  video36\n",
       "3   split_23.mp4  video36\n",
       "4   split_16.mp4  video36\n",
       "5   split_36.mp4  video36\n",
       "6    split_4.mp4  video36\n",
       "7    split_1.mp4  video36\n",
       "8    split_2.mp4  video36\n",
       "9   split_11.mp4  video36\n",
       "10  split_33.mp4  video36\n",
       "11  split_17.mp4  video36\n",
       "12  split_18.mp4  video36\n",
       "13   split_6.mp4  video36\n",
       "14  split_26.mp4  video36\n",
       "15  split_12.mp4  video36\n",
       "16  split_31.mp4  video36\n",
       "17  split_28.mp4  video36\n",
       "18   split_9.mp4  video36\n",
       "19  split_27.mp4  video36\n",
       "20   split_0.mp4  video36\n",
       "21  split_34.mp4  video36\n",
       "22  split_14.mp4  video36\n",
       "23  split_30.mp4  video36\n",
       "24  split_29.mp4  video36\n",
       "25   split_3.mp4  video36\n",
       "26   split_8.mp4  video36\n",
       "27  split_25.mp4  video36\n",
       "28  split_10.mp4  video36\n",
       "29  split_38.mp4  video36\n",
       "30  split_22.mp4  video36\n",
       "31  split_19.mp4  video36\n",
       "32  split_32.mp4  video36\n",
       "33  split_21.mp4  video36\n",
       "34  split_24.mp4  video36\n",
       "35  split_37.mp4  video36\n",
       "36  split_13.mp4  video36\n",
       "37  split_15.mp4  video36\n",
       "38  split_20.mp4  video36"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>folder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>split_5.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>split_35.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>split_7.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>split_23.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>split_16.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>split_36.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>split_4.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>split_1.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>split_2.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>split_11.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>split_33.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>split_17.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>split_18.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>split_6.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>split_26.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>split_12.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>split_31.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>split_28.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>split_9.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>split_27.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>split_0.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>split_34.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>split_14.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>split_30.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>split_29.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>split_3.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>split_8.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>split_25.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>split_10.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>split_38.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>split_22.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>split_19.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>split_32.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>split_21.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>split_24.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>split_37.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>split_13.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>split_15.mp4</td>\n      <td>video36</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>split_20.mp4</td>\n      <td>video36</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "server = 'personal'\n",
    "server = DotMap(load_yaml('../../config/servers.yml')[server])\n",
    "\n",
    "database_name = 'cholec80_splits'\n",
    "database_df = recursive_scan2df(os.path.join(server.database.location, database_name), postfix='.mp4')\n",
    "database_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # individual database post-processing\n",
    "# # database_df = database_df[database_df.folder != 'videos']\n",
    "# database_df = database_df[database_df.folder != 'sample_videos']\n",
    "# # database_df = database_df.iloc[:1]\n",
    "# database_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:116: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "\n",
      "Looking for cholec80_splits_metadata.pkl\n",
      "Found.\n",
      "../../utils/processing/endoscopy/endoscopy/processing.py:187: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  hsv = hsv.astype(np.float)/hsv.max()\n",
      "Illumination level: 0.9990148900856151, threshold: True../../utils/processing/endoscopy/endoscopy/processing.py:187: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  hsv = hsv.astype(np.float)/hsv.max()\n",
      "Illumination level: 0.9991513957462971, threshold: True"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import VideoDataset\n",
    "from utils.io import load_pickle, save_pickle\n",
    "from utils.processing import endoscopy\n",
    "\n",
    "video_paths = [os.path.join(server.database.location, database_name, x.folder, x.file) for _, x in database_df.iterrows()]\n",
    "frame_rate = 25\n",
    "duration = 60\n",
    "clip_length_in_frames = int(frame_rate*duration)\n",
    "num_workers = 8\n",
    "\n",
    "# load metadata, if existing\n",
    "metadata_name = 'cholec80_splits_metadata.pkl'\n",
    "metadata = None\n",
    "print('\\nLooking for {}'.format(metadata_name))\n",
    "if os.path.exists(metadata_name):\n",
    "    metadata = load_pickle(metadata_name)\n",
    "    print('Found.')\n",
    "else:\n",
    "    print('Could not find {}. Generating metadata.'.format(metadata_name))\n",
    "\n",
    "video_ds = VideoDataset(\n",
    "            video_paths=video_paths,\n",
    "            clip_length_in_frames=clip_length_in_frames,\n",
    "            frames_between_clips=clip_length_in_frames,\n",
    "            frame_rate=frame_rate,\n",
    "            precomputed_metadata=metadata,\n",
    "            num_workers=num_workers,\n",
    "            permute=False,\n",
    "            convert_dtype=False\n",
    ")\n",
    "\n",
    "# store metadata\n",
    "if not os.path.exists(metadata_name):\n",
    "    metadata = video_ds.metadata\n",
    "    save_pickle(metadata_name, metadata)\n",
    "\n",
    "# prepare database\n",
    "database_transforms = pd.DataFrame(\n",
    "    columns=['database', 'train', 'file', 'pre_transforms', 'aug_transforms', 'auxiliary']\n",
    ")\n",
    "\n",
    "# boundary detection parameters\n",
    "illumination_th = 0.998\n",
    "\n",
    "# iterate through each video\n",
    "for batch in video_ds:\n",
    "    video, augmented_video, frame_rate, video_fps, video_idx, idx = batch\n",
    "    video_path = database_df.iloc[video_idx].folder\n",
    "    video_name = database_df.iloc[video_idx].file\n",
    "\n",
    "    for frame in video:\n",
    "        frame = frame.numpy()\n",
    "\n",
    "        # pre-process frame\n",
    "        mask = endoscopy.bilateralSegmentation(frame, th=0.07)\n",
    "        center, radius = endoscopy.boundary_detection.ransacBoundaryCircle(mask, fit='numeric', n_iter=1)\n",
    "\n",
    "        if radius is not None:\n",
    "            illumination = endoscopy.illuminationLevel(mask, center, radius)\n",
    "\n",
    "            print('\\rIllumination level: {}, threshold: {}'.format(illumination, illumination > 0.998), end='')\n",
    "\n",
    "            top_left, shape = endoscopy.maxRectangleInCircle(frame.shape, center, radius)\n",
    "            center, radius = center.astype(int), int(radius)\n",
    "            top_left, shape = top_left.astype(int), tuple(map(int, shape))\n",
    "            cv2.circle(frame, (center[1], center[0]), radius, (255, 255, 0), 2)\n",
    "            cv2.rectangle(frame, (top_left[1], top_left[0]), (top_left[1] + shape[1], top_left[0] + shape[0]), (255, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('img', frame[...,::-1])\n",
    "            cv2.imshow('mask', mask)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "            if illumination > illumination_th:\n",
    "                row = {\n",
    "                    'database': 'cholec80_splits',\n",
    "                    'train': True,\n",
    "                    'file': {\n",
    "                        'name': video_name,\n",
    "                        'path': video_path\n",
    "                    },\n",
    "                    'pre_transforms': [\n",
    "                        {\n",
    "                            'module': 'utils.transforms',\n",
    "                            'type': 'Crop',\n",
    "                            'kwargs': {\n",
    "                                'top_left_corner': [top_left[0], top_left[1]],\n",
    "                                'shape': [shape[0], shape[1]],\n",
    "                                'order': 'chw'\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            'module': 'torchvision.transforms',\n",
    "                            'type': 'Resize',\n",
    "                            'kwargs': {\n",
    "                                'size': [240, 320]\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    'aug_transforms': [],\n",
    "                    'auxiliary': {}\n",
    "                }\n",
    "\n",
    "                database_transforms = database_transforms.append(row, ignore_index=True)\n",
    "                break\n",
    "\n",
    "# database_transforms.to_pickle('cholec80_splits_transforms.pkl')\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'VideoDataModule' object has no attribute '_has_setup_fit'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6e4b76518a6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_setup_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# train_md, _, _ = dm.setup()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hil/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py\u001b[0m in \u001b[0;36mhas_setup_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcalled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_setup_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VideoDataModule' object has no attribute '_has_setup_fit'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotmap import DotMap\n",
    "\n",
    "from utils.io import load_yaml\n",
    "from utils.io import load_pickle, save_pickle\n",
    "from utils.transforms import anyDictListToCompose\n",
    "from lightning_data_modules import VideoDataModule\n",
    "\n",
    "\n",
    "meta_df_name = 'cholec80_splits_transforms.pkl'\n",
    "meta_df = pd.read_pickle(meta_df_name)\n",
    "meta_df.aug_transforms = None  # no transforms for evaluation\n",
    "\n",
    "# load cholec80 splits\n",
    "prefix = server.database.location\n",
    "clip_length_in_frames = 2\n",
    "frames_between_clips = 1\n",
    "frame_rate = 5\n",
    "train_split = 0.5\n",
    "batch_size = 1\n",
    "num_workers = 8\n",
    "random_state = 42\n",
    "\n",
    "dm = VideoDataModule(\n",
    "    meta_df,\n",
    "    prefix=prefix,\n",
    "    clip_length_in_frames=clip_length_in_frames,\n",
    "    frames_between_clips=frames_between_clips   ,\n",
    "    frame_rate=frame_rate,\n",
    "    train_split=train_split,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# train_md, _, _ = dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n",
      "/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/io/video.py:105: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
      "  \"The pts_unit 'pts' gives wrong results and will be removed in a \"\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../../datasets/video_dataset.py\", line 77, in __getitem__\n    video = self._pre_transforms[video_idx](video)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 244, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 319, in resize\n    raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\nTypeError: img should be PIL Image. Got <class 'torch.Tensor'>\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-656fe2484363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_vid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_fps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hil/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"../../datasets/video_dataset.py\", line 77, in __getitem__\n    video = self._pre_transforms[video_idx](video)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/transforms/transforms.py\", line 244, in __call__\n    return F.resize(img, self.size, self.interpolation)\n  File \"/home/martin/anaconda3/envs/hil/lib/python3.7/site-packages/torchvision/transforms/functional.py\", line 319, in resize\n    raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\nTypeError: img should be PIL Image. Got <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from kornia import tensor_to_image\n",
    "\n",
    "train_dl = dm.train_dataloader()\n",
    "\n",
    "for batch in train_dl:\n",
    "    vid, aug_vid, frame_rate, video_fps, video_idx, idx = batch\n",
    "\n",
    "    for frame in vid:\n",
    "        cv2.imshow('frame', tensor_to_image(frame))\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}