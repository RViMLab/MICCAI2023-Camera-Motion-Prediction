{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/envs/torch110/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/martin/miniconda3/envs/torch110/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning: Failed to load image Python extension: /home/martin/miniconda3/envs/torch110/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file from ../../config/servers.yml\n",
      "Reading file from /media/martin/Samsung_T5/logs/miccai/feature_lstm/phantom/resnet34/frame_increment_1/transforms/version_0/config.yml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "\n",
    "from lightning_modules import FeatureLSTMIncrementalModule\n",
    "from utils.io import load_yaml\n",
    "\n",
    "\n",
    "server = \"local\"\n",
    "servers = load_yaml(\"../../config/servers.yml\")\n",
    "server = servers[server]\n",
    "\n",
    "log_location = \"miccai/feature_lstm/phantom/resnet34/frame_increment_1/transforms/version_0\"\n",
    "checkpoint = \"checkpoints/epoch=15-step=320.ckpt\"\n",
    "\n",
    "config = load_yaml(os.path.join(server[\"logging\"][\"location\"], log_location, \"config.yml\"))\n",
    "model = FeatureLSTMIncrementalModule.load_from_checkpoint(os.path.join(server[\"logging\"][\"location\"], log_location, checkpoint), **config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       folder           file vid frame                  center     radius  \\\n",
      "746   split_1    frame_0.npy   1     0    [558.6992, 976.5541]  470.07944   \n",
      "756   split_1   frame_10.npy   1    10   [562.67975, 976.1036]  471.65958   \n",
      "766   split_1   frame_20.npy   1    20   [562.67975, 976.1036]  471.65958   \n",
      "776   split_1   frame_30.npy   1    30   [562.44214, 976.4827]  472.15582   \n",
      "786   split_1   frame_40.npy   1    40   [562.44214, 976.4827]  472.15582   \n",
      "...       ...            ...  ..   ...                     ...        ...   \n",
      "6658  split_8  frame_690.npy   8   690    [551.8678, 984.4424]   471.7767   \n",
      "6668  split_8  frame_700.npy   8   700    [551.8678, 984.4424]   471.7767   \n",
      "6678  split_8  frame_710.npy   8   710  [550.41907, 984.63464]   471.4694   \n",
      "6688  split_8  frame_720.npy   8   720  [550.41907, 984.63464]   471.4694   \n",
      "6698  split_8  frame_730.npy   8   730  [550.41907, 984.63464]   471.4694   \n",
      "\n",
      "      train                                                duv   duv_mpd  \n",
      "746   False  [[1.7118467092514038, 1.2413957118988037], [1....  2.740309  \n",
      "756   False  [[-0.7176280617713928, 0.5880852341651917], [-...  1.951233  \n",
      "766   False  [[1.7895667552947998, -1.203918218612671], [0....  1.948142  \n",
      "776   False  [[-0.666420042514801, 0.6757497787475586], [-2...  1.921220  \n",
      "786   False  [[-0.2938310503959656, -0.2926788032054901], [...  1.798347  \n",
      "...     ...                                                ...       ...  \n",
      "6658  False  [[-0.4631364941596985, 3.3968544006347656], [0...  3.756168  \n",
      "6668  False  [[-1.4406659603118896, 6.255959510803223], [6....  4.973784  \n",
      "6678  False  [[-0.815049946308136, 1.693662405014038], [-1....  2.068291  \n",
      "6688  False  [[-1.125579595565796, 0.8737807869911194], [1....  3.390779  \n",
      "6698  False  [[1.6544289588928223, -0.15563639998435974], [...  4.283792  \n",
      "\n",
      "[148 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.processing import differentiate_duv\n",
    "\n",
    "prefix = \"21_11_25_first_test_data_frames\"\n",
    "file = \"pre_processed_10th_frame_log_test_train_no_nan.pkl\"\n",
    "df = pd.read_pickle(\n",
    "    os.path.join(server[\"database\"][\"location\"], prefix, file)\n",
    ")\n",
    "df = df[df.train == False]\n",
    "print(df)\n",
    "\n",
    "# from datasets import ImageSequenceDuvDataset\n",
    "# ds = ImageSequenceDuvDataset(\n",
    "#     df,\n",
    "#     os.path.join(server[\"database\"][\"location\"], prefix),\n",
    "#     seq_len=20,\n",
    "#     frame_increment=1,\n",
    "#     frames_between_clips=20\n",
    "# )\n",
    "\n",
    "\n",
    "# from kornia import tensor_to_image\n",
    "# from utils.viz import create_blend_from_four_point_homography\n",
    "# from utils.io import generate_path\n",
    "\n",
    "\n",
    "# for imgs, duvs, frame_idx, vid_idx in ds:\n",
    "#     output_path = f\"/tmp/blends/vid_{vid_idx}\"\n",
    "#     generate_path(output_path)\n",
    "#     imgs = imgs.unsqueeze(0).to(model.device).float()/255.\n",
    "#     duvs = duvs.unsqueeze(0).to(model.device).float()\n",
    "    \n",
    "#     # derivative\n",
    "#     dduvs = differentiate_duv(duvs)\n",
    "\n",
    "#     duvs_ip1 = model(imgs[:, 2:], duvs[:, 1:-1], dduvs[:, :-1])\n",
    "\n",
    "#     print(f\"\\\n",
    "#         \\tduvs shape : {duvs.shape}\\n\\\n",
    "#         \\tdduvs shape: {dduvs.shape}\\n\\\n",
    "#         \\tduvs_ip1 shape: {duvs_ip1.shape}\\n\\\n",
    "#         \\timgs shape: {imgs.shape}\\n\\\n",
    "#     \")\n",
    "    \n",
    "#     batch = 0\n",
    "#     blends = create_blend_from_four_point_homography(imgs[batch, 2:-1], imgs[batch, 3:], duvs_ip1[batch,:-1])\n",
    "\n",
    "#     for idx, blend in enumerate(blends):\n",
    "#         blend = tensor_to_image(blend, False)\n",
    "#         blend = (blend*255.).astype(np.uint8)\n",
    "#         blend = cv2.resize(blend, [640, 480])\n",
    "        \n",
    "#         # cv2.imwrite(f\"{output_path}/blend_{idx}.png\", blend)\n",
    "#         cv2.imshow(\"blend\", blend)\n",
    "#         cv2.waitKey()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de43f3610355f051a4a7d1ec68e5cd39983800d0bb5000cb4a591287222bab46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
