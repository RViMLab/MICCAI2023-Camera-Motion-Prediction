{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Motion Statistical Analysis\n",
    "Analyzes video clips for frame to frame motion and bins the severity of motion into a histogram."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Iterate over test set and safe duv into dataframe.\n",
    "\n",
    "- Create test loader\n",
    "- Load homography estimation model from configs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Batch: 6/300, BxNxCxHxW: [1, 2, 3, 480, 640]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotmap import DotMap\n",
    "from torch.utils.data import DataLoader\n",
    "from kornia import tensor_to_image, warp_perspective\n",
    "\n",
    "import lightning_modules\n",
    "from datasets import VideoDataset\n",
    "from utils.io import load_yaml, load_pickle, save_pickle, scan2df, natural_keys\n",
    "from utils.transforms import anyDictListToCompose\n",
    "from utils.processing import image_edges, four_point_homography_to_matrix\n",
    "from utils.viz import yt_alpha_blend\n",
    "\n",
    "# iterate over test set and safe duv into dataframe\n",
    "server = 'local'\n",
    "servers = load_yaml('../configs/servers.yml')\n",
    "server = DotMap(servers[server])\n",
    "\n",
    "configs = 'next_view.yml'\n",
    "configs = load_yaml(os.path.join(server.configs.location, configs))\n",
    "\n",
    "backbone_path = 'deep_image_homography_estimation_backbone/version_2'\n",
    "\n",
    "# append configs by backbone\n",
    "backbone_configs = load_yaml(os.path.join(server.logging.location, backbone_path, 'configs.yml'))\n",
    "df = scan2df(os.path.join(server.logging.location, backbone_path, 'checkpoints'), '.ckpt')\n",
    "ckpts = sorted(list(df.file), key=natural_keys)\n",
    "configs['model']['homography_regression'] = {\n",
    "    'lightning_module': backbone_configs['lightning_module'],\n",
    "    'model': backbone_configs['model'],\n",
    "    'path': backbone_path,\n",
    "    'checkpoint': 'checkpoints/{}'.format(ckpts[-1]),\n",
    "    'experiment': backbone_configs['experiment']\n",
    "}\n",
    "\n",
    "configs = DotMap(configs)\n",
    "\n",
    "meta_df = pd.read_pickle(os.path.join(server.configs.location, configs.data.meta_df))\n",
    "test_meta_df = meta_df[meta_df.train == False]\n",
    "test_video_paths = [os.path.join(server.database.location, row.database, row.file['path'], row.file['name']) for _, row in test_meta_df.iterrows()]\n",
    "test_pre_transforms = [anyDictListToCompose(row.pre_transforms) for _, row in test_meta_df.iterrows()]\n",
    "test_aug_transforms = [anyDictListToCompose(row.aug_transforms) for _, row in test_meta_df.iterrows()]\n",
    "\n",
    "# load video meta data if existing, returns None if none existent\n",
    "test_metadata = load_pickle(os.path.join(server.configs.location, configs.data.test_metadata))\n",
    "if test_metadata is not None:\n",
    "    if len(test_metadata['video_paths']) == 0:\n",
    "        test_metadata = None\n",
    "\n",
    "# create video dataset and dataloader\n",
    "clip_length_in_frames = 2\n",
    "frame_rate = 5\n",
    "frames_between_clips = 1\n",
    "precomputed_metadata = test_metadata\n",
    "num_workers = 1\n",
    "pre_transforms = test_pre_transforms\n",
    "aug_transforms = test_aug_transforms\n",
    "seeds = True\n",
    "\n",
    "vid_ds = VideoDataset(\n",
    "    video_paths=test_video_paths,\n",
    "    clip_length_in_frames=clip_length_in_frames,\n",
    "    frames_between_clips=frames_between_clips,\n",
    "    frame_rate=frame_rate,\n",
    "    precomputed_metadata=test_metadata,\n",
    "    num_workers=num_workers,\n",
    "    pre_transforms=pre_transforms,\n",
    "    aug_transforms=aug_transforms,\n",
    "    seeds=seeds\n",
    ")\n",
    "\n",
    "# save metadata\n",
    "if test_metadata is None:\n",
    "    save_pickle(os.path.join(server.configs.location, configs.data.test_metadata), vid_ds.metadata)\n",
    "\n",
    "batch_size = 1\n",
    "drop_last = True\n",
    "\n",
    "vid_dl = DataLoader(\n",
    "    vid_ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=drop_last\n",
    ")\n",
    "\n",
    "# load model\n",
    "homography_regression = getattr(lightning_modules, configs.model.homography_regression.lightning_module).load_from_checkpoint(\n",
    "    checkpoint_path=os.path.join(server.logging.location, configs.model.homography_regression.path, configs.model.homography_regression.checkpoint),\n",
    "    **configs.model\n",
    ")\n",
    "homography_regression.eval()\n",
    "\n",
    "for batch_idx, batch in enumerate(vid_dl):\n",
    "    vids, aug_vids, re_sample_frame_rates, frame_rates, vid_idc = batch\n",
    "    print('\\rBatch: {}/{}, BxNxCxHxW: {}'.format(batch_idx + 1, len(vid_dl), list(vids.shape)), end='')\n",
    "\n",
    "    frames_i, frames_ips = vids[:,0], vids[:,1]  # N = 2\n",
    "\n",
    "    frames_i   = frames_i.view((-1,) + frames_i.shape[-3:])      # BxNxCxHxW -> B*NxCxHxW\n",
    "    frames_ips = frames_ips.view((-1,) + frames_ips.shape[-3:])  # BxNxCxHxW -> B*NxCxHxW\n",
    "    duvs = homography_regression(frames_i, frames_ips)\n",
    "\n",
    "\n",
    "\n",
    "#     # viz\n",
    "#     uvs = image_edges(frames_i)         \n",
    "#     Hs = four_point_homography_to_matrix(uvs, duvs)\n",
    "#     wrps = warp_perspective(frames_i, torch.inverse(Hs), frames_i.shape[-2:])\n",
    "#     blends = yt_alpha_blend(frames_ips, wrps)\n",
    "\n",
    "#     for blend in blends:\n",
    "#         cv2.imshow('blend', tensor_to_image(blend))\n",
    "#         cv2.waitKey()\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate histogram of the norm of duv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample videos with supposedly little and much motion\n"
   ]
  }
 ]
}