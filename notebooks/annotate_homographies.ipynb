{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Annotate Homographies\n",
    "Large scale homography annotation for extraction of next best views."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.io as io\n",
    "from torchvision.datasets.video_utils import VideoClips\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import load_yaml, save_yaml, dictListToCompose, recursiveMethodCallFromDictList\n",
    "\n",
    "servers = load_yaml('../configs/servers.yml')\n",
    "server = servers['local']\n",
    "\n",
    "databases = load_yaml('../configs/cholec80_transforms.yml')\n",
    "database = databases['databases'][0]"
   ]
  },
  {
   "source": [
    "# load transforms and convert them to torch.functional methods\n",
    "key_dict = {\n",
    "    'Crop': 'crop',\n",
    "    'Resize': 'resize'\n",
    "}\n",
    "\n",
    "functional_databases = copy.deepcopy(databases)\n",
    "for db_idx, db in enumerate(databases['databases']):\n",
    "    functional_databases['databases'][db_idx]['transforms'] = [] # delete old transforms\n",
    "    for transforms in db['transforms']:\n",
    "        functional_transforms = []\n",
    "        for transform in transforms:\n",
    "            # 'Crop' -> 'crop', shape -> height, width, top_left_corner -> top, left\n",
    "            # 'Resize' -> 'resize', dsize -> size\n",
    "            functional_transform = {}\n",
    "            for key, value in transform.items():\n",
    "                if key == 'Crop':\n",
    "                    functional_transform[key_dict[key]] = {\n",
    "                        'height': value['shape'][0],\n",
    "                        'width': value['shape'][1],\n",
    "                        'top': value['top_left_corner'][0],\n",
    "                        'left': value['top_left_corner'][1]\n",
    "                    }\n",
    "                elif key == 'Resize':\n",
    "                    functional_transform[key_dict[key]] = {\n",
    "                        'size': value['dsize'][::-1]\n",
    "                    }\n",
    "                else:\n",
    "                    raise ValueError('Key not known')\n",
    "\n",
    "            functional_transforms.append(functional_transform)                \n",
    "        functional_databases['databases'][db_idx]['transforms'].append(functional_transforms)\n",
    "\n",
    "save_yaml('../configs/cholec80_transforms_functional.yml', functional_databases)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running with CUDA backend.\n"
     ]
    }
   ],
   "source": [
    "import kornia\n",
    "import cv2\n",
    "from kornia import warp_perspective\n",
    "\n",
    "from utils.processing import image_edges, four_pt_to_matrix_homography_representation\n",
    "from lightning_modules import DeepImageHomographyEstimationModuleBackbone\n",
    "\n",
    "# load the model\n",
    "# load best model\\n\",\n",
    "model_prefix = '/home/martin/Tresors/homography_imitation_learning_logs/deep_image_homography_estimation_backbone/version_2'\n",
    "# prefix = '/home/martin/Tresors/homography_imitation_learning_logs/unsupervised_deep_homography_estimation_backbone/version_0'\n",
    "configs = load_yaml(os.path.join(model_prefix, 'configs.yml'))\n",
    "model = DeepImageHomographyEstimationModuleBackbone.load_from_checkpoint(os.path.join(model_prefix, 'checkpoints/epoch=49.ckpt'), shape=configs['model']['shape'])\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('Running with CUDA backend.')\n",
    "    device = 'cuda'\n",
    "\n",
    "model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "def forward_model(frame_i, frame_ip1):\n",
    "    duv = model(frame_i, frame_ip1)\n",
    "\n",
    "    uv = image_edges(frame_i)\n",
    "    H = four_pt_to_matrix_homography_representation(uv, duv)\n",
    "\n",
    "    wrp = warp_perspective(frame_i, torch.inverse(H), frame_i.shape[-2:])\n",
    "\n",
    "    return wrp, H, duv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "\n",
      "Loading time: 121.820683 ms\n",
      "Forwarding time: 693.775682 ms\n",
      "25/750\n",
      "Loading time: 230.959205 ms\n",
      "Forwarding time: 132.851764 ms\n",
      "50/750\n",
      "Loading time: 289.967595 ms\n",
      "Forwarding time: 130.481043 ms\n",
      "75/750\n",
      "Loading time: 339.831279 ms\n",
      "Forwarding time: 146.180757 ms\n",
      "100/750\n",
      "Loading time: 405.020069 ms\n",
      "Forwarding time: 139.023227 ms\n",
      "125/750\n",
      "Loading time: 447.353381 ms\n",
      "Forwarding time: 134.475889 ms\n",
      "150/750\n",
      "Loading time: 519.782804 ms\n",
      "Forwarding time: 137.43477 ms\n",
      "175/750\n",
      "Loading time: 576.220349 ms\n",
      "Forwarding time: 134.853256 ms\n",
      "200/750\n",
      "Loading time: 666.922047 ms\n",
      "Forwarding time: 133.928237 ms\n",
      "225/750\n",
      "Loading time: 719.402735 ms\n",
      "Forwarding time: 144.60392 ms\n",
      "250/750\n",
      "Loading time: 759.020998 ms\n",
      "Forwarding time: 136.39896 ms\n",
      "275/750\n",
      "Loading time: 204.74555 ms\n",
      "Forwarding time: 141.343443 ms\n",
      "300/750\n",
      "Loading time: 269.396871 ms\n",
      "Forwarding time: 139.789613 ms\n",
      "325/750\n",
      "Loading time: 338.602279 ms\n",
      "Forwarding time: 142.911266 ms\n",
      "350/750\n",
      "Loading time: 382.046954 ms\n",
      "Forwarding time: 129.585263 ms\n",
      "375/750\n",
      "Loading time: 436.155009 ms\n",
      "Forwarding time: 131.758919 ms\n",
      "400/750\n",
      "Loading time: 516.222907 ms\n",
      "Forwarding time: 139.32728 ms\n",
      "425/750\n",
      "Loading time: 579.021836 ms\n",
      "Forwarding time: 135.465643 ms\n",
      "450/750\n",
      "Loading time: 671.383375 ms\n",
      "Forwarding time: 137.084102 ms\n",
      "475/750\n",
      "Loading time: 700.485921 ms\n",
      "Forwarding time: 135.502654 ms\n",
      "500/750\n",
      "Loading time: 751.831432 ms\n",
      "Forwarding time: 129.914676 ms\n",
      "525/750\n",
      "Loading time: 189.450978 ms\n",
      "Forwarding time: 140.837786 ms\n",
      "550/750\n",
      "Loading time: 225.700387 ms\n",
      "Forwarding time: 133.058375 ms\n",
      "575/750\n",
      "Loading time: 266.672714 ms\n",
      "Forwarding time: 134.665295 ms\n",
      "600/750\n",
      "Loading time: 307.264321 ms\n",
      "Forwarding time: 86.510279 ms\n",
      "625/750\n",
      "Loading time: 391.675213 ms\n",
      "Forwarding time: 104.384158 ms\n",
      "650/750\n",
      "Loading time: 419.721786 ms\n",
      "Forwarding time: 86.272209 ms\n",
      "675/750\n",
      "Loading time: 522.192818 ms\n",
      "Forwarding time: 88.70471 ms\n",
      "700/750\n",
      "Loading time: 513.001447 ms\n",
      "Forwarding time: 84.690789 ms\n",
      "725/750\n",
      "Loading time: 529.345481 ms\n",
      "Forwarding time: 87.955273 ms\n",
      "750/750"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.viz import yt_alpha_blend\n",
    "import time\n",
    "\n",
    "# load the transforms and videos from database\n",
    "functional_databases = load_yaml('../configs/cholec80_transforms_functional.yml')\n",
    "functional_database = functional_databases['databases'][0]\n",
    "\n",
    "paths = [\n",
    "    os.path.join(\n",
    "        server['database']['location'], \n",
    "        functional_database['prefix'], \n",
    "        functional_database['videos']['prefix'], \n",
    "        x\n",
    "    ) for x in functional_database['videos']['files'][:1]\n",
    "]\n",
    "\n",
    "paths = ['/media/martin/Samsung_T5/data/endoscopic_data/cholec80/videos/video01_short.mp4']\n",
    "\n",
    "# video reader not compiled yet https://github.com/pytorch/vision/issues/1446\n",
    "# video reading https://github.com/pytorch/vision/blob/ed5b2dc3a5e7411d8b40cc7e526e151983e99cf9/torchvision/datasets/video_utils.py#L45-L69\n",
    "# dataset example check https://github.com/pytorch/vision/blob/ed5b2dc3a5e7411d8b40cc7e526e151983e99cf9/torchvision/datasets/kinetics.py#L50-L78\n",
    "\n",
    "N = 100 # already 4 second preview horizon via N = 100 @ full res, image downscale 2x2 -> factor 4, easily 16 seconds preview horizon with 8GB memory\n",
    "step = 5\n",
    "\n",
    "vc = VideoClips(paths, clip_length_in_frames=N+1, frames_between_clips=N)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['t', 'duv', 'H']) # track results\n",
    "global_idx = 0\n",
    "max_clips = 150 # vc.num_clips()\n",
    "max_clips = int(min(max_clips, vc.num_clips()))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_clips):\n",
    "        now = time.time_ns()\n",
    "        video, audio, info, video_idx = vc.get_clip(i)\n",
    "        print('\\nLoading time: {} ms'.format((time.time_ns() - now)/1.e6))\n",
    "\n",
    "        video = video.permute(0, 3, 1, 2)\n",
    "        transforms = functional_database['transforms'][video_idx]\n",
    "        video = recursiveMethodCallFromDictList(video, transforms, torchvision.transforms.functional)\n",
    "        video = video.float()/255.\n",
    "\n",
    "        # re-sort images into i and i+1\n",
    "        frames_i   = video[:-step:step]\n",
    "        frames_ip1 = video[step::step]\n",
    "\n",
    "        frames_i, frames_ip1 = frames_i.to(device), frames_ip1.to(device)\n",
    "\n",
    "        now = time.time_ns()\n",
    "        wrps, Hs, duvs = forward_model(frames_i, frames_ip1)\n",
    "        print('Forwarding time: {} ms'.format((time.time_ns() - now)/1.e6))\n",
    "\n",
    "        for idx, H in enumerate(Hs):\n",
    "            df = df.append({\n",
    "                't': global_idx,\n",
    "                'duv': duvs[idx].squeeze().cpu().numpy(),\n",
    "                'H': H.squeeze().cpu().numpy()\n",
    "            }, ignore_index=True)\n",
    "            global_idx += step\n",
    "            print('\\r{}/{}'.format(global_idx, max_clips*N), end='')\n",
    "\n",
    "        frames_i = kornia.tensor_to_image(frames_i)\n",
    "        frames_ip1 = kornia.tensor_to_image(frames_ip1)\n",
    "        wrps = kornia.tensor_to_image(wrps)\n",
    "\n",
    "        for idx, w in enumerate(wrps):\n",
    "            blend = yt_alpha_blend(frames_ip1[idx], w)\n",
    "            cv2.imshow('blend', blend)\n",
    "            cv2.imshow('img', frames_i[idx])\n",
    "            cv2.imshow('wrp', w)\n",
    "            cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "df.to_pickle('H_{}.pkl'.format(step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}