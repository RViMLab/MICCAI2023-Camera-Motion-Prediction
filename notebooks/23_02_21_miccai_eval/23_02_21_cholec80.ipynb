{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Homography Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file from /tmp/chin/resnet50/version_2/config.yml\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheckpoints/epoch=0-step=352.ckpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m config \u001b[39m=\u001b[39m load_yaml(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_prefix, \u001b[39m\"\u001b[39m\u001b[39mconfig.yml\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m module \u001b[39m=\u001b[39m ConvHomographyPredictorModule\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m     20\u001b[0m     os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(checkpoint_prefix, checkpoint), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     21\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch110/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:139\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     67\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:  \u001b[39m# type: ignore[valid-type]\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_from_checkpoint(\n\u001b[1;32m    140\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m    141\u001b[0m         checkpoint_path,\n\u001b[1;32m    142\u001b[0m         map_location,\n\u001b[1;32m    143\u001b[0m         hparams_file,\n\u001b[1;32m    144\u001b[0m         strict,\n\u001b[1;32m    145\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch110/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:160\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     map_location \u001b[39m=\u001b[39m cast(_MAP_LOCATION_TYPE, \u001b[39mlambda\u001b[39;00m storage, loc: storage)\n\u001b[1;32m    159\u001b[0m \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m--> 160\u001b[0m     checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m    162\u001b[0m \u001b[39m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m    163\u001b[0m checkpoint \u001b[39m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m    164\u001b[0m     checkpoint, checkpoint_path\u001b[39m=\u001b[39m(checkpoint_path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(checkpoint_path, (\u001b[39mstr\u001b[39m, Path)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch110/lib/python3.9/site-packages/lightning_fabric/utilities/cloud_io.py:48\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     46\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39mopen(path_or_url, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(f, map_location\u001b[39m=\u001b[39;49mmap_location)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch110/lib/python3.9/site-packages/torch/serialization.py:777\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m     \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m     \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m     \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m     orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n\u001b[0;32m--> 777\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_reader(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    778\u001b[0m         \u001b[39mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m    779\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    780\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m dispatching to \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (call \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch.jit.load\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directly to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    781\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m silence this warning)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch110/lib/python3.9/site-packages/torch/serialization.py:282\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name_or_buffer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_reader, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileReader(name_or_buffer))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.io import load_yaml\n",
    "from lightning_modules.homography_imitation import ConvHomographyPredictorModule\n",
    "\n",
    "\n",
    "# checkpoint_prefix = \"/home/martin/Tresors/homography_imitation_learning_logs/miccai/cross_val/cholec80/resnet34/version_0\"\n",
    "# checkpoint = \"checkpoints/epoch=48-step=172578.ckpt\"\n",
    "checkpoint_prefix = \"/tmp/chin/resnet50/version_2\"\n",
    "checkpoint = \"checkpoints/epoch=0-step=352.ckpt\"\n",
    "\n",
    "\n",
    "config = load_yaml(os.path.join(checkpoint_prefix, \"config.yml\"))\n",
    "\n",
    "module = ConvHomographyPredictorModule.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_prefix, checkpoint), **config[\"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de43f3610355f051a4a7d1ec68e5cd39983800d0bb5000cb4a591287222bab46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
