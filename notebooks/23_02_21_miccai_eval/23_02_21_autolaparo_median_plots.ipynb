{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AutoLaparo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.io import load_yaml\n",
    "\n",
    "preview_horizon = 1\n",
    "seq_len = 15\n",
    "frame_increment = 5\n",
    "frames_between_clips = 1\n",
    "\n",
    "server = \"local\"\n",
    "server = load_yaml(\"../../config/servers.yml\")[server]\n",
    "database = \"autolaparo_single_frames/AutoLaparo_Task2\"\n",
    "prefix = os.path.join(server[\"database\"][\"location\"], database)\n",
    "# pkl = f\"23_03_03_pre_processed_frame_increment_{frame_increment}_frames_between_clips_1_log.pkl\"\n",
    "pkl = f\"23_03_03_motion_label_window_1_frame_increment_{frame_increment}_frames_between_clips_1_log_test_train.pkl\"\n",
    "\n",
    "vid_df = pd.read_pickle(os.path.join(prefix, pkl))\n",
    "\n",
    "# sort!\n",
    "vid_df[[\"vid\", \"frame\"]] = vid_df[[\"vid\", \"frame\"]].astype(float)\n",
    "vid_df = vid_df.sort_values(by=[\"vid\", \"frame\"]).reset_index(drop=True)\n",
    "vid_df[[\"vid\", \"frame\"]] = vid_df[[\"vid\", \"frame\"]].astype(int)\n",
    "\n",
    "label_df = pd.read_csv(os.path.join(prefix, \"laparoscope_motion_label.csv\"))\n",
    "label_df.Clip = label_df.Clip.apply(lambda x: x-1)\n",
    "\n",
    "print(vid_df)\n",
    "print(label_df)\n",
    "\n",
    "# remove data that is irrelevant to autolaparo\n",
    "length = len(vid_df[vid_df[\"vid\"] == 0])\n",
    "print(f\"initial length: {length}\")\n",
    "\n",
    "# get last half plus sequence length\n",
    "# print(len(vid_df)) 250 frames, of which we take half. Increment is missing due to pre-processing\n",
    "# vid_df = vid_df.groupby(\"vid\").tail(int(length/2) + seq_len - preview_horizon).reset_index(drop=True)\n",
    "# vid_df = vid_df.groupby(\"vid\").iloc[125 - seq_len*frame_increment:200].reset_index(drop=True)\n",
    "vid_df = vid_df[vid_df.frame >= 120 - (seq_len-1)*frame_increment]\n",
    "vid_df = vid_df[vid_df.frame < 120 + (seq_len-5)*frame_increment].reset_index(drop=True)\n",
    "\n",
    "print(vid_df)\n",
    "\n",
    "\n",
    "length = len(vid_df[vid_df[\"vid\"] == 0])\n",
    "print(f\"length after: {length}\")\n",
    "\n",
    "# from readme\n",
    "motion_dict = {\n",
    "    0: \"Static\",\n",
    "    1: \"Up\",\n",
    "    2: \"Down\",\n",
    "    3: \"Left\",\n",
    "    4: \"Right\",\n",
    "    5: \"Zoom-in\",\n",
    "    6: \"Zoom-out\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Homography Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning_modules.homography_imitation import ConvHomographyPredictorModule\n",
    "\n",
    "\n",
    "def cholec80(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/cholec80/resnet18/version_2\"\n",
    "        checkpoint = \"checkpoints/epoch=39-step=78400.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/cholec80/resnet34/version_2\"\n",
    "        checkpoint = \"checkpoints/epoch=67-step=133280.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/cholec80/resnet50/version_2\"\n",
    "        checkpoint = \"checkpoints/epoch=62-step=123480.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "def heichole(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/heichole/resnet18/version_1\"\n",
    "        checkpoint = \"checkpoints/epoch=32-step=11715.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/heichole/resnet34/version_3\"\n",
    "        checkpoint = \"checkpoints/epoch=48-step=17395.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/heichole/resnet50/version_1\"\n",
    "        checkpoint = \"checkpoints/epoch=36-step=13135.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "def autolaparo(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/autolaparo/resnet18/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=26-step=3159.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/autolaparo/resnet34/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=41-step=4914.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/autolaparo/resnet50/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=48-step=5733.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "def phantom(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/phantom/resnet18/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=138-step=834.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/phantom/resnet34/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=62-step=378.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/media/martin/Samsung_T5/logs/miccai/final/phantom/resnet50/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=39-step=240.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "checkpoint_prefix, checkpoint = cholec80(34)\n",
    "# checkpoint_prefix, checkpoint = heichole(50)\n",
    "# checkpoint_prefix, checkpoint = phantom(18)\n",
    "\n",
    "config = load_yaml(os.path.join(checkpoint_prefix, \"config.yml\"))\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "predictor = ConvHomographyPredictorModule.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_prefix, checkpoint), **config[\"model\"]\n",
    ")\n",
    "predictor.to(device)\n",
    "predictor = predictor.eval()\n",
    "predictor.freeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Homography Estimator and Taylor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.processing import TaylorHomographyPrediction\n",
    "from lightning_modules.homography_regression import DeepImageHomographyEstimationModuleBackbone\n",
    "\n",
    "def ae_cai():\n",
    "    checkpoint_prefix = \"/media/martin/Samsung_T5/logs/ae_cai/resnet/48/25/34/version_0\"\n",
    "    checkpoint = \"checkpoints/epoch=99-step=47199.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "\n",
    "checkpoint_prefix_estimator, checkpoint_estimator = ae_cai()\n",
    "\n",
    "estimator_config = load_yaml(os.path.join(checkpoint_prefix_estimator, \"config.yml\"))\n",
    "\n",
    "estimator = DeepImageHomographyEstimationModuleBackbone.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_prefix_estimator, checkpoint_estimator), **estimator_config[\"model\"]\n",
    ")\n",
    "estimator.to(device)\n",
    "estimator = estimator.eval()\n",
    "estimator.freeze()\n",
    "\n",
    "taylor_predictor = TaylorHomographyPrediction(order=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Homographies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import ImageSequenceMotionLabelDataset, ImageSequenceDataset\n",
    "import tqdm\n",
    "\n",
    "from utils.transforms import dict_list_to_augment\n",
    "\n",
    "import PIL\n",
    "from IPython.display import display, clear_output\n",
    "from kornia import tensor_to_image\n",
    "\n",
    "output_path = \"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_06_trained_model_pred_on_autolaparo\"\n",
    "\n",
    "transforms = dict_list_to_augment([\n",
    "    {\"chance\": 1.0, \"module\": \"imgaug.augmenters\", \"type\": \"Resize\", \"kwargs\": {\"size\": {\"height\": 240, \"width\": 320}}}\n",
    "])\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df=vid_df,\n",
    "    prefix=prefix,\n",
    "    seq_len=seq_len,\n",
    "    frame_increment=frame_increment,\n",
    "    frames_between_clips=seq_len*frame_increment,\n",
    "    geometric_transforms=transforms,\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "results = []\n",
    "for batch in tqdm.tqdm(dl):\n",
    "    # pre-process\n",
    "    imgs, imgs_tf, frame_idcs, vid_idcs = batch\n",
    "    B, T, C, H, W = imgs.shape\n",
    "    preview_horizon_imgs = imgs[:, :-preview_horizon]\n",
    "    for img in preview_horizon_imgs[0]:\n",
    "        display(PIL.Image.fromarray(tensor_to_image(img, keepdim=False)))\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import ImageSequenceMotionLabelDataset, ImageSequenceDataset\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from utils.transforms import dict_list_to_augment\n",
    "from utils.viz import create_blend_from_four_point_homography\n",
    "from utils.processing import classify_duv_motion, frame_pairs\n",
    "\n",
    "output_path = \"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_trained_model_pred_on_autolaparo\"\n",
    "\n",
    "transforms = dict_list_to_augment([\n",
    "    {\"chance\": 1.0, \"module\": \"imgaug.augmenters\", \"type\": \"Resize\", \"kwargs\": {\"size\": {\"height\": 240, \"width\": 320}}}\n",
    "])\n",
    "\n",
    "delta = 0\n",
    "frame_increment = 5\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df=vid_df,\n",
    "    prefix=prefix,\n",
    "    seq_len=seq_len + delta,\n",
    "    frame_increment=frame_increment,\n",
    "    frames_between_clips=frame_increment, # frames_between_clips,\n",
    "    geometric_transforms=transforms,\n",
    "    random_frame_offset=False\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=False, num_workers=4, drop_last=True)\n",
    "\n",
    "results = []\n",
    "sigma = 11.\n",
    "for batch in tqdm.tqdm(dl):\n",
    "    # pre-process\n",
    "    imgs, imgs_tf, frame_idcs, vid_idcs = batch\n",
    "    B, T, C, H, W = imgs.shape\n",
    "    imgs = imgs.to(device).float() / 255.\n",
    "    recall_horizon_imgs = imgs[:, :-delta-preview_horizon]\n",
    "    recall_horizon_imgs = recall_horizon_imgs.reshape(B, -1, H, W)\n",
    "    recall_horizon_imgs = recall_horizon_imgs\n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        duvs_predicted = predictor(recall_horizon_imgs)\n",
    "\n",
    "    # estimate camera motion for taylor\n",
    "    imgs_i, imgs_ip1 = frame_pairs(recall_horizon_imgs.view(B, -1, C, H, W))\n",
    "    with torch.no_grad():\n",
    "        duvs_estimated = estimator(imgs_i.reshape(-1, C, H, W), imgs_ip1.reshape(-1, C, H, W))\n",
    "        duvs_estimated = duvs_estimated.view(B, T-1-preview_horizon, 4, 2)\n",
    "\n",
    "    # taylor predict camera motion\n",
    "    duvs_taylor_predicted = taylor_predictor(duvs_estimated.cpu())[:, -preview_horizon:].squeeze()\n",
    "\n",
    "    # visualize\n",
    "    # recall_horizon_imgs = recall_horizon_imgs.reshape(B, -1, C, H, W)\n",
    "    # blends = create_blend_from_four_point_homography(recall_horizon_imgs[:, -1], imgs[:, -1], duvs)\n",
    "\n",
    "    # import PIL\n",
    "    # from IPython.display import display, clear_output, TextDisplayObject\n",
    "    # from kornia import tensor_to_image\n",
    "    # from kornia.geometry import resize\n",
    "    # import numpy as np\n",
    "    # for blend, vid_idx in zip(blends, vid_idcs):\n",
    "    #     motion = motion_dict[label_df[label_df[\"Clip\"] == vid_idx.item()][\"Label\"].values[0]]\n",
    "    #     display(PIL.Image.fromarray((tensor_to_image(resize(blend, [480, 640]), keepdim=False) * 255.).astype(np.uint8)))\n",
    "    #     display(TextDisplayObject(motion).data)\n",
    "    #     clear_output(wait=True)\n",
    "\n",
    "    # break\n",
    "\n",
    "    duvs_mpd = np.linalg.norm(duvs_predicted.cpu().numpy(), axis=-1).mean(axis=-1)\n",
    "\n",
    "    # logs duvs with video label\n",
    "    for duv_predicted, duv_taylor_predicted, duv_mpd, vid_idx in zip(duvs_predicted, duvs_taylor_predicted, duvs_mpd, vid_idcs):\n",
    "        # or classify motion here! and keep only directed\n",
    "        # if duv_mpd > 10: # discard noisy / undecisive motions for binary classification\n",
    "\n",
    "        duv_predicted = duv_predicted.cpu().numpy().tolist()\n",
    "        motion = classify_duv_motion(duv_predicted[0][0], duv_predicted[0][1], duv_predicted[1][0], duv_predicted[1][1], duv_predicted[2][0], duv_predicted[2][1], duv_predicted[3][0], duv_predicted[3][1], motion_threadhold=1.0*sigma)\n",
    "        label = motion_dict[label_df[label_df[\"Clip\"] == vid_idx.item()][\"Label\"].values[0]]\n",
    "\n",
    "        result = {\n",
    "            \"vid\": vid_idx.item(),\n",
    "            \"duv_predicted\": np.nan,\n",
    "            \"duv_taylor_predicted\": np.nan,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "        if motion != \"static\":\n",
    "            result[\"duv_predicted\"] = duv_predicted\n",
    "\n",
    "        duv_taylor_predicted = duv_taylor_predicted.cpu().numpy().tolist()\n",
    "        motion = classify_duv_motion(duv_taylor_predicted[0][0], duv_taylor_predicted[0][1], duv_taylor_predicted[1][0], duv_taylor_predicted[1][1], duv_taylor_predicted[2][0], duv_taylor_predicted[2][1], duv_taylor_predicted[3][0], duv_taylor_predicted[3][1], motion_threadhold=1.0*sigma)\n",
    "        if motion != \"static\":\n",
    "            result[\"duv_taylor_predicted\"] = duv_taylor_predicted\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "name = \"_\".join(config[\"experiment\"].split(\"/\")[-2:])\n",
    "print(name)\n",
    "results_df.to_pickle(os.path.join(output_path, f\"23_03_07_autolaparo_{name}.pkl\"))\n",
    "results_df.to_csv(os.path.join(output_path, f\"23_03_07_autolaparo_{name}.csv\"))\n",
    "results_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Classify Homographies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "from utils.processing import classify_duv_motion\n",
    "\n",
    "datasets = [\"cholec80\"]\n",
    "# backbones = [\"resnet18\", \"resnet34\", \"resnet50\"]\n",
    "backbones = [\"resnet34\"]\n",
    "\n",
    "def to_path(datset, backbone, prefix=\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval\"):\n",
    "    return os.path.join(prefix, f\"23_03_07_trained_model_pred_on_autolaparo/23_03_07_autolaparo_{datset}_{backbone}.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duv to homography\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "shape = [240, 320]\n",
    "\n",
    "# center point\n",
    "p = np.array([int((shape[0]-1)/2), int((shape[1]-1)/2)]).astype(np.float32)\n",
    "\n",
    "img_edges = np.array([\n",
    "    [0, 0],\n",
    "    [0, shape[1]],\n",
    "    [shape[0], shape[1]],\n",
    "    [shape[0], 0],\n",
    "])\n",
    "\n",
    "def center_point_classifier(duv: np.array) -> list:\n",
    "    if np.isnan(duv).any():\n",
    "        return [np.nan, np.nan]\n",
    "    wrp_edges = img_edges + duv\n",
    "    h = cv2.getPerspectiveTransform(img_edges.astype(np.float32), wrp_edges.astype(np.float32))\n",
    "    \n",
    "    # transform center under homography\n",
    "    p_prime = cv2.perspectiveTransform(p.reshape(-1, 1, 2), h)\n",
    "    \n",
    "    # get delta\n",
    "    duv_center = p_prime - p\n",
    "    return duv_center[0][0].tolist()\n",
    "\n",
    "def edge_classifier(duv: np.array) -> list:\n",
    "    return duv.mean(axis=0).tolist()\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for backbone in backbones:\n",
    "        print(backbone)\n",
    "        df = pd.read_pickle(to_path(dataset, backbone))\n",
    "        duvs_predicted_center = []\n",
    "        duvs_taylor_predicted_center = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            duvs_predicted_center.append(center_point_classifier(np.array(row.duv_predicted)))\n",
    "            duvs_taylor_predicted_center.append(center_point_classifier(np.array(row.duv_taylor_predicted)))\n",
    "            # duvs_center.append(edge_classifier(row.duv))\n",
    "\n",
    "        # log delta p\n",
    "        df[\"duv_predicted_center\"] = duvs_predicted_center\n",
    "        df[\"duv_taylor_predicted_center\"] = duvs_taylor_predicted_center\n",
    "\n",
    "        print(df)\n",
    "\n",
    "        df.to_csv(to_path(dataset, backbone).split(\".\")[0] + \"_duv_center.csv\")\n",
    "        df.to_pickle(to_path(dataset, backbone).split(\".\")[0] + \"_duv_center.pkl\")\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import seaborn\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['text.usetex'] = True\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "datasets = [\"cholec80\"]\n",
    "# backbones = [\"resnet18\", \"resnet34\", \"resnet50\"]\n",
    "backbones = [\"resnet18\", \"resnet34\", \"resnet50\"]\n",
    "\n",
    "backbone_rename = {\n",
    "    \"resnet18\": \"ResNet-18\",\n",
    "    \"resnet34\": \"ResNet-34\",\n",
    "    \"resnet50\": \"Resnet-50\",\n",
    "}\n",
    "\n",
    "def to_path(datset, backbone, prefix=\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval\"):\n",
    "    return os.path.join(prefix, f\"23_03_07_trained_model_pred_on_autolaparo/23_03_07_autolaparo_{datset}_{backbone}_duv_center.pkl\")\n",
    "\n",
    "# target_labels = [\"Up\", \"Down\", \"Left\", \"Right\", \"Zoom-in\", \"Zoom-out\"]\n",
    "target_labels = [\"Up\", \"Down\", \"Left\", \"Right\"]\n",
    "\n",
    "seaborn.set_style(\"whitegrid\")\n",
    "# seaborn.set(font_scale=1.5, rc={'text.usetex' : True})\n",
    "for dataset in datasets:\n",
    "    for backbone in backbones:\n",
    "        df = pd.read_pickle(to_path(dataset, backbone))\n",
    "\n",
    "        # try:\n",
    "        #     grouped = df.groupby(\"label\")\n",
    "        #     df.drop(grouped.get_group(\"Zoom-in\").index, inplace=True)\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        # try:\n",
    "        #     grouped = df.groupby(\"label\")\n",
    "        #     df.drop(grouped.get_group(\"Zoom-out\").index, inplace=True)\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        try:\n",
    "            grouped = df.groupby(\"label\")\n",
    "            df.drop(grouped.get_group(\"Static\").index, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        seaborn.set(font_scale=1.2)\n",
    "        seaborn.set_palette(\"husl\", n_colors=4)\n",
    "\n",
    "        df[\"duv_predicted_x\"] = df.duv_predicted_center.apply(lambda x: x[1]/120. if not np.isnan(x).any() else np.nan)\n",
    "        df[\"duv_predicted_y\"] = df.duv_predicted_center.apply(lambda x: -x[0]/160 if not np.isnan(x).any() else np.nan)\n",
    "        df[\"duv_taylor_predicted_x\"] = df.duv_taylor_predicted_center.apply(lambda x: x[1]/120. if not np.isnan(x).any() else np.nan)\n",
    "        df[\"duv_taylor_predicted_y\"] = df.duv_taylor_predicted_center.apply(lambda x: -x[0]/160 if not np.isnan(x).any() else np.nan)\n",
    "\n",
    "\n",
    "\n",
    "        df.rename(columns={\"duv_predicted_x\": backbone_rename[backbone], \"duv_taylor_predicted_x\": \"O(1)-Taylor\", \"label\": \"AutoLaparo\"}, inplace=True)\n",
    "        merged_df = pd.melt(df, id_vars=[\"AutoLaparo\"], value_vars=[backbone_rename[backbone], \"O(1)-Taylor\"], var_name=\"Method\", value_name=\"Predicted x-axis camera motion [a.u.]\")\n",
    "        seaborn.boxplot(x=\"Predicted x-axis camera motion [a.u.]\", y=\"Method\", data=merged_df, hue=\"AutoLaparo\", order=[\"O(1)-Taylor\", backbone_rename[backbone]], hue_order=target_labels, width=0.4, fliersize=2)\n",
    "        plt.axvline(x=0, color=\"black\", linestyle=\"--\")\n",
    "        plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_x.pdf\", bbox_inches='tight')\n",
    "        plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_x.png\", bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n",
    "        # repeat for y\n",
    "        df.rename(columns={backbone_rename[backbone]: \"duv_predicted_x\", \"O(1)-Taylor\": \"duv_taylor_predicted_x\"}, inplace=True)\n",
    "        df.rename(columns={\"duv_predicted_y\": backbone_rename[backbone], \"duv_taylor_predicted_y\": \"O(1)-Taylor\"}, inplace=True)\n",
    "        merged_df = pd.melt(df, id_vars=[\"AutoLaparo\"], value_vars=[backbone_rename[backbone], \"O(1)-Taylor\"], var_name=\"Method\", value_name=\"Predicted y-axis camera motion [a.u.]\")\n",
    "        seaborn.boxplot(x=\"Method\", y=\"Predicted y-axis camera motion [a.u.]\", data=merged_df, hue=\"AutoLaparo\", order=[\"O(1)-Taylor\", backbone_rename[backbone]], hue_order=target_labels, width=0.4, fliersize=2)\n",
    "        plt.axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "        plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_y.pdf\", bbox_inches='tight')\n",
    "        plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_y.png\", bbox_inches='tight')\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "        # # seaborn.violinplot(x=\"duv_x\", y=\"label\", gridsize=200, data=df)\n",
    "        # # seaborn.boxplot(x=\"duv_x\", y=\"label\", data=df[df.label.isin([\"Left\", \"Right\"])], width=0.2, palette={\"Left\": \"C2\", \"Right\": \"C3\"}, order=[\"Left\", \"Right\"])\n",
    "        # seaborn.boxplot(x=\"duv_predicted_x\", y=\"label\", data=df[[\"duv_predicted_x\", \"duv_taylor_predicted_x\", \"label\"]][df.label.isin([\"Left\", \"Right\", \"Up\", \"Down\"])], width=0.2, order=[\"Left\", \"Right\", \"Up\", \"Down\"])\n",
    "        # # seaborn.boxplot(x=\"duv_taylor_predicted_x\", y=\"label\", data=df[[\"duv_taylor_predicted_x\", \"label\"]].dropna()[df.label.isin([\"Left\", \"Right\", \"Up\", \"Down\"])], width=0.2, order=[\"Left\", \"Right\", \"Up\", \"Down\"])\n",
    "        # # plt.title(\"\")\n",
    "        # plt.xlabel(\"Predicted camera motion along x-axis / a.u.\")\n",
    "        # plt.ylabel(\"AutoLaparo label\")\n",
    "        # plt.axvline(0, linestyle=\"--\", color=\"black\")\n",
    "        # plt.xlim([-0.9, 0.9])\n",
    "        # # plt.show()\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_x.pdf\", bbox_inches='tight')\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_x.png\", bbox_inches='tight')\n",
    "        # plt.clf()\n",
    "        # # seaborn.violinplot(x=\"label\", y=\"duv_y\", gridsize=200, data=df)\n",
    "        # # seaborn.boxplot(x=\"label\", y=\"duv_y\", data=df[df.label.isin([\"Up\", \"Down\"])], width=0.2, palette={\"Up\": \"C0\", \"Down\": \"C1\"}, order=[\"Up\", \"Down\"])\n",
    "        # seaborn.boxplot(x=\"label\", y=\"duv_predicted_y\", data=df[[\"duv_predicted_y\", \"label\"]][df.label.isin([\"Left\", \"Right\", \"Up\", \"Down\"])], width=0.2, order=[\"Left\", \"Right\", \"Up\", \"Down\"])\n",
    "        # plt.xlabel(\"AutoLaparo label\") \n",
    "        # plt.ylabel(\"Predicted camera motion along y-axis / a.u.\")\n",
    "        # plt.axhline(0, linestyle=\"--\", color=\"black\")\n",
    "        # plt.ylim([-0.5, 0.5])\n",
    "        # # plt.show()\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_y.pdf\", bbox_inches='tight')\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_{dataset}_{backbone}_duv_center_y.png\", bbox_inches='tight')\n",
    "        # plt.clf()\n",
    "\n",
    "        # seaborn.boxplot(x=\"duv_taylor_predicted_x\", y=\"label\", data=df[[\"duv_taylor_predicted_x\", \"label\"]].dropna()[df.label.isin([\"Left\", \"Right\", \"Up\", \"Down\"])], width=0.2, order=[\"Left\", \"Right\", \"Up\", \"Down\"])\n",
    "        # # plt.title(\"\")\n",
    "        # plt.xlabel(\"Predicted camera motion along x-axis / a.u.\")\n",
    "        # plt.ylabel(\"AutoLaparo label\")\n",
    "        # plt.axvline(0, linestyle=\"--\", color=\"black\")\n",
    "        # plt.xlim([-0.9, 0.9])\n",
    "        # # plt.show()\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_taylor_{dataset}_{backbone}_duv_center_x.pdf\", bbox_inches='tight')\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_taylor_{dataset}_{backbone}_duv_center_x.png\", bbox_inches='tight')\n",
    "        # plt.clf()\n",
    "        # # seaborn.violinplot(x=\"label\", y=\"duv_y\", gridsize=200, data=df)\n",
    "        # # seaborn.boxplot(x=\"label\", y=\"duv_y\", data=df[df.label.isin([\"Up\", \"Down\"])], width=0.2, palette={\"Up\": \"C0\", \"Down\": \"C1\"}, order=[\"Up\", \"Down\"])\n",
    "        # seaborn.boxplot(x=\"label\", y=\"duv_taylor_predicted_y\", data=df[[\"duv_taylor_predicted_y\", \"label\"]].dropna()[df.label.isin([\"Left\", \"Right\", \"Up\", \"Down\"])], width=0.2, order=[\"Left\", \"Right\", \"Up\", \"Down\"])\n",
    "        # plt.xlabel(\"AutoLaparo label\") \n",
    "        # plt.ylabel(\"Predicted camera motion along y-axis / a.u.\")\n",
    "        # plt.axhline(0, linestyle=\"--\", color=\"black\")\n",
    "        # plt.ylim([-0.5, 0.5])\n",
    "        # # plt.show()\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_taylor_{dataset}_{backbone}_duv_center_y.pdf\", bbox_inches='tight')\n",
    "        # plt.savefig(f\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_07_autolaparo_taylor_{dataset}_{backbone}_duv_center_y.png\", bbox_inches='tight')\n",
    "        # plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "        # break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "datasets = [\"heichole\"]\n",
    "# backbones = [\"resnet18\", \"resnet34\", \"resnet50\"]\n",
    "backbones = [\"resnet34\"]\n",
    "\n",
    "def to_path(datset, backbone, prefix=\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval\"):\n",
    "    return os.path.join(prefix, f\"23_03_07_trained_model_pred_on_autolaparo/23_03_07_autolaparo_{datset}_{backbone}_duv_center.pkl\")\n",
    "\n",
    "target_labels = [\"Up\", \"Down\", \"Left\", \"Right\", \"Static\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for backbone in backbones:\n",
    "        df = pd.read_pickle(to_path(dataset, backbone))\n",
    "        for label in target_labels:\n",
    "            sub_df = df[df[\"label\"] == label]\n",
    "            \n",
    "            # plot duv scatter\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.scatter(sub_df.duv_center.apply(lambda x: x[1]), sub_df.duv_center.apply(lambda x: -x[0])) # opencv coordinates to standard right handed coordinates\n",
    "            plt.title(f\"{dataset} {backbone} {label}\")\n",
    "            plt.xlim(-20, 20)\n",
    "            plt.ylim(-20, 20)\n",
    "            plt.show()\n",
    "\n",
    "            print(\"x_mean: \", sub_df.duv_center.apply(lambda x: x[1]).mean(), \" x_std: \", sub_df.duv_center.apply(lambda x: x[1]).std())\n",
    "            print(\"y_mean: \", sub_df.duv_center.apply(lambda x: -x[0]).mean(), \" y_std: \", sub_df.duv_center.apply(lambda x: -x[0]).std())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duv_label() -> List[str]:\n",
    "    return [f\"duv_{i}_{j}\" for i in range(4) for j in range(2)]\n",
    "\n",
    "def split_duv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_split = df\n",
    "    df_split[\"duv\"] = df[\"duv\"].apply(\n",
    "        lambda x: np.array(x).flatten()\n",
    "    )\n",
    "    df_split = pd.DataFrame(\n",
    "        df_split[\"duv\"].to_list(), columns=duv_label()\n",
    "    )\n",
    "\n",
    "    df_split = pd.concat(\n",
    "        [df.reset_index(drop=True), df_split], axis=1\n",
    "    )\n",
    "\n",
    "    return df_split\n",
    "\n",
    "# from readme\n",
    "motion_dict_other = {\n",
    "    0: \"static\",\n",
    "    1: \"up\",\n",
    "    2: \"down\",\n",
    "    3: \"left\",\n",
    "    4: \"right\",\n",
    "    5: \"zoom_in\",\n",
    "    6: \"zoom_out\",\n",
    "}\n",
    "\n",
    "true_cnt = 0\n",
    "false_cnt = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    for backbone in backbones:\n",
    "        df = pd.read_pickle(to_path(dataset, backbone))\n",
    "        \n",
    "        df = split_duv(df)\n",
    "        df[\"label\"] = df.apply(lambda x: classify_duv_motion(x.duv_0_0, x.duv_0_1, x.duv_1_0, x.duv_1_1, x.duv_2_0, x.duv_2_1, x.duv_3_0, x.duv_3_1, motion_threadhold=7), axis=1)\n",
    "\n",
    "        for name, group in df.groupby(by=\"vid\"):\n",
    "            autolaparo_label = motion_dict_other[label_df[label_df[\"Clip\"] == name][\"Label\"].values[0]]\n",
    "            labels = group.label.value_counts()\n",
    "            try:\n",
    "                labels = labels.drop(index=[\"static\"])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                labels = labels.drop(index=[\"mixture\"])\n",
    "            except:\n",
    "                pass\n",
    "            if len(labels) > 1:\n",
    "                # print(labels.index[0], autolaparo_label)\n",
    "                if labels.index[0] == autolaparo_label or labels.index[1] == autolaparo_label: # first most common\n",
    "                    true_cnt += 1\n",
    "                else:\n",
    "                    false_cnt += 1\n",
    "\n",
    "        print(backbone, true_cnt, false_cnt, true_cnt / (true_cnt + false_cnt))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de43f3610355f051a4a7d1ec68e5cd39983800d0bb5000cb4a591287222bab46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
