{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AutoLaparo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.io import load_yaml\n",
    "\n",
    "seq_len = 15\n",
    "increment = 5\n",
    "\n",
    "server = \"local\"\n",
    "server = load_yaml(\"../../config/servers.yml\")[server]\n",
    "database = \"autolaparo_single_frames/AutoLaparo_Task2\"\n",
    "prefix = os.path.join(server[\"database\"][\"location\"], database)\n",
    "pkl = f\"23_03_03_motion_label_window_1_frame_increment_{increment}_frames_between_clips_1_log_test_train.pkl\"\n",
    "# pkl = f\"23_03_03_motion_label_window_1_frame_increment_{increment}_frames_between_clips_1_log.pkl\"\n",
    "# pkl = f\"23_03_03_pre_processed_frame_increment_{increment}_frames_between_clips_1_log.pkl\"\n",
    "\n",
    "\n",
    "vid_df = pd.read_pickle(os.path.join(prefix, pkl))\n",
    "\n",
    "# sort!\n",
    "vid_df[[\"vid\", \"frame\"]] = vid_df[[\"vid\", \"frame\"]].astype(float)\n",
    "vid_df = vid_df.sort_values(by=[\"vid\", \"frame\"]).reset_index(drop=True)\n",
    "vid_df[[\"vid\", \"frame\"]] = vid_df[[\"vid\", \"frame\"]].astype(int)\n",
    "\n",
    "length = len(vid_df[vid_df[\"vid\"] == 0])\n",
    "\n",
    "# get last half plus sequence length\n",
    "# print(len(vid_df)) 250 frames, of which we take half. Increment is missing due to pre-processing\n",
    "vid_df = vid_df.groupby(\"vid\").tail(125 -increment + seq_len ).reset_index(drop=True)\n",
    "# print(len(vid_df))\n",
    "\n",
    "label_df = pd.read_csv(os.path.join(prefix, \"laparoscope_motion_label.csv\"))\n",
    "\n",
    "print(vid_df)\n",
    "print(label_df)\n",
    "\n",
    "# from readme\n",
    "motion_dict = {\n",
    "    0: \"Static\",\n",
    "    1: \"Up\",\n",
    "    2: \"Down\",\n",
    "    3: \"Left\",\n",
    "    4: \"Right\",\n",
    "    5: \"Zoom-in\",\n",
    "    6: \"Zoom-out\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Homography Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kornia import tensor_to_image\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from utils.io import load_yaml\n",
    "from datasets import ImageSequenceDataset\n",
    "\n",
    "\n",
    "server = \"local\"\n",
    "server = load_yaml(\"../../config/servers.yml\")[server]\n",
    "database = \"autolaparo_single_frames/AutoLaparo_Task2\"\n",
    "prefix = os.path.join(server[\"database\"][\"location\"], database)\n",
    "pkl = \"log.pkl\"\n",
    "\n",
    "df = pd.read_pickle(os.path.join(prefix, pkl))\n",
    "df_a = df[df[\"vid\"] == 298]\n",
    "df_b = df[df[\"vid\"] == 299]\n",
    "\n",
    "df = pd.concat([df_a, df_b]).reset_index(drop=True)\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df,\n",
    "    prefix,\n",
    "    seq_len=10,\n",
    "    frame_increment=5,\n",
    "    frames_between_clips=50,\n",
    ")\n",
    "\n",
    "ds._df = ds._df.astype(object)\n",
    "dl = DataLoader(\n",
    "    ds,\n",
    "    num_workers=0,\n",
    "    batch_size=1,\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "from kornia.geometry import resize\n",
    "\n",
    "for batch in dl:\n",
    "    imgs, tf_imgs, frame_idcs, vid_idcs = batch\n",
    "    imgs = imgs.float()/255.\n",
    "    imgs = resize(imgs, (240, 320))\n",
    "    print(imgs.shape)\n",
    "    # print(frame_idcs)\n",
    "\n",
    "    # for img in imgs[0]: # batch 0\n",
    "    #     img = tensor_to_image(img, keepdim=False).astype(np.uint8)\n",
    "    #     display(PIL.Image.fromarray(img))\n",
    "    #     clear_output(wait=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_modules.homography_imitation import ConvHomographyPredictorModule\n",
    "\n",
    "\n",
    "checkpoint_prefix = \"/tmp/miccai/cholec80/resnet18/version_0\"\n",
    "checkpoint = \"checkpoints/epoch=65-step=129360.ckpt\"\n",
    "\n",
    "config = load_yaml(os.path.join(checkpoint_prefix, \"config.yml\"))\n",
    "\n",
    "module = ConvHomographyPredictorModule.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_prefix, checkpoint), **config[\"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kornia import tensor_to_image\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import ImageSequenceDataset\n",
    "\n",
    "from utils.viz import create_blend_from_four_point_homography\n",
    "from utils.transforms import dict_list_to_augment\n",
    "\n",
    "import PIL\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "transforms = dict_list_to_augment([\n",
    "    {\"chance\": 1.0, \"module\": \"imgaug.augmenters\", \"type\": \"Resize\", \"kwargs\": {\"size\": {\"height\": 240, \"width\": 320}}}\n",
    "])\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df=vid_df,\n",
    "    prefix=prefix,\n",
    "    seq_len=15,\n",
    "    frame_increment=10,\n",
    "    frames_between_clips=100,\n",
    "    geometric_transforms=transforms,\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "for batch in dl:\n",
    "    imgs, imgs_tf, frame_idcs, vid_idx = batch\n",
    "    B, T, C, H, W = imgs.shape\n",
    "    preview_horizon = imgs[:, :-1]\n",
    "    preview_horizon = preview_horizon.float() / 255.\n",
    "    preview_horizon = preview_horizon.reshape(B, -1, H, W)\n",
    "\n",
    "    duvs = module(preview_horizon)\n",
    "    print(duvs)\n",
    "    \n",
    "    # for img in imgs[0]: # batch 0\n",
    "    #     img = (tensor_to_image(img, keepdim=False)*255.).astype(np.uint8)\n",
    "    #     display(PIL.Image.fromarray(img))\n",
    "    #     clear_output(wait=True)\n",
    "\n",
    "    break\n",
    "\n",
    "    # input = imgs[:, :-1]\n",
    "    # input = input.reshape(B, -1, H, W)\n",
    "    # duv = module(input)*100\n",
    "\n",
    "    # # direction lsos\n",
    "    # blend = create_blend_from_four_point_homography(imgs[:, -2], imgs[:, -1], duv)\n",
    "    # blend = (tensor_to_image(blend, keepdim=False) * 255.).astype(np.uint8)\n",
    "    \n",
    "    # blend = cv2.resize(blend, (640, 480))\n",
    "    # cv2.imwrite(f\"/tmp/chin/dara/{idx}.png\", blend)\n",
    "    # idx += 1\n",
    "    # if idx >= 100:\n",
    "    #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de43f3610355f051a4a7d1ec68e5cd39983800d0bb5000cb4a591287222bab46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
