{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AutoLaparo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils.io import load_yaml\n",
    "\n",
    "preview_horizon = 1\n",
    "seq_len = 15\n",
    "frame_increment = 5\n",
    "frames_between_clips = 1\n",
    "\n",
    "server = \"local\"\n",
    "server = load_yaml(\"../../config/servers.yml\")[server]\n",
    "database = \"autolaparo_single_frames/AutoLaparo_Task2\"\n",
    "prefix = os.path.join(server[\"database\"][\"location\"], database)\n",
    "# pkl = f\"23_03_03_pre_processed_frame_increment_{frame_increment}_frames_between_clips_1_log.pkl\"\n",
    "pkl = f\"23_03_03_motion_label_window_1_frame_increment_{frame_increment}_frames_between_clips_1_log_test_train.pkl\"\n",
    "\n",
    "vid_df = pd.read_pickle(os.path.join(prefix, pkl))\n",
    "\n",
    "# sort!\n",
    "vid_df[[\"vid\", \"frame\"]] = vid_df[[\"vid\", \"frame\"]].astype(float)\n",
    "vid_df = vid_df.sort_values(by=[\"vid\", \"frame\"]).reset_index(drop=True)\n",
    "vid_df[[\"vid\", \"frame\"]] = vid_df[[\"vid\", \"frame\"]].astype(int)\n",
    "\n",
    "label_df = pd.read_csv(os.path.join(prefix, \"laparoscope_motion_label.csv\"))\n",
    "label_df.Clip = label_df.Clip.apply(lambda x: x-1)\n",
    "\n",
    "print(vid_df)\n",
    "print(label_df)\n",
    "\n",
    "# remove data that is irrelevant to autolaparo\n",
    "length = len(vid_df[vid_df[\"vid\"] == 0])\n",
    "print(f\"initial length: {length}\")\n",
    "\n",
    "# get last half plus sequence length\n",
    "# print(len(vid_df)) 250 frames, of which we take half. Increment is missing due to pre-processing\n",
    "# vid_df = vid_df.groupby(\"vid\").tail(int(length/2) + seq_len - preview_horizon).reset_index(drop=True)\n",
    "vid_df = vid_df.groupby(\"vid\").tail(125 - frame_increment + seq_len - preview_horizon).reset_index(drop=True)\n",
    "\n",
    "length = len(vid_df[vid_df[\"vid\"] == 0])\n",
    "print(f\"length after: {length}\")\n",
    "\n",
    "# from readme\n",
    "motion_dict = {\n",
    "    0: \"Static\",\n",
    "    1: \"Up\",\n",
    "    2: \"Down\",\n",
    "    3: \"Left\",\n",
    "    4: \"Right\",\n",
    "    5: \"Zoom-in\",\n",
    "    6: \"Zoom-out\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Homography Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning_modules.homography_imitation import ConvHomographyPredictorModule\n",
    "\n",
    "\n",
    "def cholec80(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/cholec80/resnet18/version_1\"\n",
    "        checkpoint = \"checkpoints/epoch=19-step=39200.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/cholec80/resnet34/version_1\"\n",
    "        checkpoint = \"checkpoints/epoch=19-step=39200.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/cholec80/resnet50/version_1\"\n",
    "        checkpoint = \"checkpoints/epoch=19-step=39200.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "def heichole(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/heichole/resnet18/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=40-step=14555.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/heichole/resnet34/version_1\"\n",
    "        checkpoint = \"checkpoints/epoch=34-step=12425.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/heichole/resnet50/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=41-step=14910.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "def autolaparo(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/autolaparo/resnet18/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=26-step=3159.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/autolaparo/resnet34/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=41-step=4914.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/autolaparo/resnet50/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=48-step=5733.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "def phantom(resnet: int=18):\n",
    "    if resnet == 18:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/phantom/resnet18/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=138-step=834.ckpt\"\n",
    "    elif resnet == 34:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/phantom/resnet34/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=62-step=378.ckpt\"\n",
    "    elif resnet == 50:\n",
    "        checkpoint_prefix = \"/tmp/miccai/final/phantom/resnet50/version_0\"\n",
    "        checkpoint = \"checkpoints/epoch=39-step=240.ckpt\"\n",
    "    return checkpoint_prefix, checkpoint\n",
    "\n",
    "# checkpoint_prefix, checkpoint = cholec80(34)\n",
    "checkpoint_prefix, checkpoint = heichole(18)\n",
    "# checkpoint_prefix, checkpoint = phantom(18)\n",
    "\n",
    "config = load_yaml(os.path.join(checkpoint_prefix, \"config.yml\"))\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "module = ConvHomographyPredictorModule.load_from_checkpoint(\n",
    "    os.path.join(checkpoint_prefix, checkpoint), **config[\"model\"]\n",
    ")\n",
    "module.to(device)\n",
    "module = module.eval()\n",
    "module.freeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Homographies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import ImageSequenceMotionLabelDataset, ImageSequenceDataset\n",
    "import tqdm\n",
    "\n",
    "from utils.transforms import dict_list_to_augment\n",
    "\n",
    "import PIL\n",
    "from IPython.display import display, clear_output\n",
    "from kornia import tensor_to_image\n",
    "\n",
    "output_path = \"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_06_trained_model_pred_on_autolaparo\"\n",
    "\n",
    "transforms = dict_list_to_augment([\n",
    "    {\"chance\": 1.0, \"module\": \"imgaug.augmenters\", \"type\": \"Resize\", \"kwargs\": {\"size\": {\"height\": 240, \"width\": 320}}}\n",
    "])\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df=vid_df,\n",
    "    prefix=prefix,\n",
    "    seq_len=seq_len,\n",
    "    frame_increment=frame_increment,\n",
    "    frames_between_clips=seq_len*frame_increment,\n",
    "    geometric_transforms=transforms,\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "results = []\n",
    "for batch in tqdm.tqdm(dl):\n",
    "    # pre-process\n",
    "    imgs, imgs_tf, frame_idcs, vid_idcs = batch\n",
    "    B, T, C, H, W = imgs.shape\n",
    "    preview_horizon_imgs = imgs[:, :-preview_horizon]\n",
    "    for img in preview_horizon_imgs[0]:\n",
    "        display(PIL.Image.fromarray(tensor_to_image(img, keepdim=False)))\n",
    "        clear_output(wait=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import ImageSequenceMotionLabelDataset, ImageSequenceDataset\n",
    "import tqdm\n",
    "\n",
    "from utils.transforms import dict_list_to_augment\n",
    "from utils.viz import create_blend_from_four_point_homography\n",
    "\n",
    "output_path = \"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval/23_03_06_trained_model_pred_on_autolaparo\"\n",
    "\n",
    "transforms = dict_list_to_augment([\n",
    "    {\"chance\": 1.0, \"module\": \"imgaug.augmenters\", \"type\": \"Resize\", \"kwargs\": {\"size\": {\"height\": 240, \"width\": 320}}}\n",
    "])\n",
    "\n",
    "delta = 0\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df=vid_df,\n",
    "    prefix=prefix,\n",
    "    seq_len=seq_len + delta,\n",
    "    frame_increment=frame_increment,\n",
    "    frames_between_clips=20,\n",
    "    geometric_transforms=transforms,\n",
    "    random_frame_offset=False\n",
    ")\n",
    "\n",
    "dl = DataLoader(ds, batch_size=32, shuffle=False, num_workers=4, drop_last=True)\n",
    "\n",
    "results = []\n",
    "for batch in tqdm.tqdm(dl):\n",
    "    # pre-process\n",
    "    imgs, imgs_tf, frame_idcs, vid_idcs = batch\n",
    "    B, T, C, H, W = imgs.shape\n",
    "    imgs = imgs.to(device).float() / 255.\n",
    "    preview_horizon_imgs = imgs[:, :-delta-preview_horizon]\n",
    "    preview_horizon_imgs = preview_horizon_imgs.reshape(B, -1, H, W)\n",
    "    preview_horizon_imgs = preview_horizon_imgs\n",
    "\n",
    "    # inference\n",
    "    duvs = module(preview_horizon_imgs)\n",
    "\n",
    "    # visualize\n",
    "    preview_horizon_imgs = preview_horizon_imgs.reshape(B, -1, C, H, W)\n",
    "    blends = create_blend_from_four_point_homography(preview_horizon_imgs[:, -1], imgs[:, -1], duvs)\n",
    "\n",
    "    # import PIL\n",
    "    # from IPython.display import display, clear_output\n",
    "    # from kornia import tensor_to_image\n",
    "    # from kornia.geometry import resize\n",
    "    # import numpy as np\n",
    "    # for blend in blends:\n",
    "    #     display(PIL.Image.fromarray((tensor_to_image(resize(blend, [480, 640]), keepdim=False) * 255.).astype(np.uint8)))\n",
    "    #     clear_output(wait=True)\n",
    "\n",
    "    # break\n",
    "\n",
    "    import numpy as np\n",
    "    duvs_mpd = np.linalg.norm(duvs.cpu().numpy(), axis=-1).mean(axis=-1)\n",
    "\n",
    "    # logs duvs with video label\n",
    "    for duv, duv_mpd, vid_idx in zip(duvs, duvs_mpd, vid_idcs):\n",
    "        # or classify motion here! and keep only directed\n",
    "        # if duv_mpd > 10: # discard noisy / undecisive motions for binary classification\n",
    "        label = motion_dict[label_df[label_df[\"Clip\"] == vid_idx.item()][\"Label\"].values[0]]\n",
    "        results.append({\n",
    "            \"vid\": vid_idx.item(),\n",
    "            \"duv\": duv.cpu().numpy().tolist(),\n",
    "            \"label\": label,\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "name = \"_\".join(config[\"experiment\"].split(\"/\")[-2:])\n",
    "results_df.to_pickle(os.path.join(output_path, f\"23_03_06_autolaparo_{name}.pkl\"))\n",
    "results_df.to_csv(os.path.join(output_path, f\"23_03_06_autolaparo_{name}.csv\"))\n",
    "results_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Classify Homographies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "from utils.processing import classify_duv_motion\n",
    "\n",
    "datasets = [\"phantom\"]\n",
    "# backbones = [\"resnet18\", \"resnet34\", \"resnet50\"]\n",
    "backbones = [\"resnet18\"]\n",
    "\n",
    "def to_path(datset, backbone, prefix=\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval\"):\n",
    "    return os.path.join(prefix, f\"23_03_06_trained_model_pred_on_autolaparo/23_03_06_autolaparo_{datset}_{backbone}.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duv to homography\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "shape = [240, 320]\n",
    "\n",
    "# center point\n",
    "p = np.array([int((shape[0]-1)/2), int((shape[1]-1)/2)]).astype(np.float32)\n",
    "\n",
    "img_edges = np.array([\n",
    "    [0, 0],\n",
    "    [0, shape[1]],\n",
    "    [shape[0], shape[1]],\n",
    "    [shape[0], 0],\n",
    "])\n",
    "\n",
    "def center_point_classifier(duv: np.array) -> list:\n",
    "    duv = np.array(row.duv)\n",
    "    wrp_edges = img_edges + duv\n",
    "    h = cv2.getPerspectiveTransform(img_edges.astype(np.float32), wrp_edges.astype(np.float32))\n",
    "    \n",
    "    # transform center under homography\n",
    "    p_prime = cv2.perspectiveTransform(p.reshape(-1, 1, 2), h)\n",
    "    \n",
    "    # get delta\n",
    "    duv_center = p_prime - p\n",
    "    return duv_center[0][0].tolist()\n",
    "\n",
    "def edge_classifier(duv: np.array) -> list:\n",
    "    duv = np.array(duv)\n",
    "    return duv.mean(axis=0).tolist()\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for backbone in backbones:\n",
    "        print(backbone)\n",
    "        df = pd.read_pickle(to_path(dataset, backbone))\n",
    "        duvs_center = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            duvs_center.append(center_point_classifier(row.duv))\n",
    "            # duvs_center.append(edge_classifier(row.duv))\n",
    "\n",
    "        # log delta p\n",
    "        df[\"duv_center\"] = duvs_center\n",
    "\n",
    "        df.to_csv(to_path(dataset, backbone).split(\".\")[0] + \"_duv_center.csv\")\n",
    "        df.to_pickle(to_path(dataset, backbone).split(\".\")[0] + \"_duv_center.pkl\")\n",
    "    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "datasets = [\"phantom\"]\n",
    "# backbones = [\"resnet18\", \"resnet34\", \"resnet50\"]\n",
    "backbones = [\"resnet18\"]\n",
    "\n",
    "def to_path(datset, backbone, prefix=\"/media/martin/Samsung_T5/23_02_20_miccai_measurements/eval\"):\n",
    "    return os.path.join(prefix, f\"23_03_06_trained_model_pred_on_autolaparo/23_03_06_autolaparo_{datset}_{backbone}_duv_center.pkl\")\n",
    "\n",
    "target_labels = [\"Up\", \"Down\", \"Left\", \"Right\", \"Static\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for backbone in backbones:\n",
    "        df = pd.read_pickle(to_path(dataset, backbone))\n",
    "        for label in target_labels:\n",
    "            sub_df = df[df[\"label\"] == label]\n",
    "            \n",
    "            # plot duv scatter\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.scatter(sub_df.duv_center.apply(lambda x: x[1]), sub_df.duv_center.apply(lambda x: -x[0]))\n",
    "            plt.title(f\"{dataset} {backbone} {label}\")\n",
    "            # plt.xlim(-20, 20)\n",
    "            # plt.ylim(-20, 20)\n",
    "            plt.show()\n",
    "\n",
    "            print(\"x_mean: \", sub_df.duv_center.apply(lambda x: x[1]).mean())\n",
    "            print(\"y_mean: \", sub_df.duv_center.apply(lambda x: -x[0]).mean())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duv_label() -> List[str]:\n",
    "    return [f\"duv_{i}_{j}\" for i in range(4) for j in range(2)]\n",
    "\n",
    "def split_duv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_split = df\n",
    "    df_split[\"duv\"] = df[\"duv\"].apply(\n",
    "        lambda x: np.array(x).flatten()\n",
    "    )\n",
    "    df_split = pd.DataFrame(\n",
    "        df_split[\"duv\"].to_list(), columns=duv_label()\n",
    "    )\n",
    "\n",
    "    df_split = pd.concat(\n",
    "        [df.reset_index(drop=True), df_split], axis=1\n",
    "    )\n",
    "\n",
    "    return df_split\n",
    "\n",
    "# from readme\n",
    "motion_dict_other = {\n",
    "    0: \"static\",\n",
    "    1: \"up\",\n",
    "    2: \"down\",\n",
    "    3: \"left\",\n",
    "    4: \"right\",\n",
    "    5: \"zoom_in\",\n",
    "    6: \"zoom_out\",\n",
    "}\n",
    "\n",
    "true_cnt = 0\n",
    "false_cnt = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    for backbone in backbones:\n",
    "        df = pd.read_pickle(to_path(dataset, backbone))\n",
    "        \n",
    "        df = split_duv(df)\n",
    "        df[\"label\"] = df.apply(lambda x: classify_duv_motion(x.duv_0_0, x.duv_0_1, x.duv_1_0, x.duv_1_1, x.duv_2_0, x.duv_2_1, x.duv_3_0, x.duv_3_1, motion_threadhold=7), axis=1)\n",
    "\n",
    "        for name, group in df.groupby(by=\"vid\"):\n",
    "            autolaparo_label = motion_dict_other[label_df[label_df[\"Clip\"] == name][\"Label\"].values[0]]\n",
    "            labels = group.label.value_counts()\n",
    "            try:\n",
    "                labels = labels.drop(index=[\"static\"])\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                labels = labels.drop(index=[\"mixture\"])\n",
    "            except:\n",
    "                pass\n",
    "            if len(labels) > 1:\n",
    "                # print(labels.index[0], autolaparo_label)\n",
    "                if labels.index[0] == autolaparo_label or labels.index[1] == autolaparo_label: # first most common\n",
    "                    true_cnt += 1\n",
    "                else:\n",
    "                    false_cnt += 1\n",
    "\n",
    "        print(backbone, true_cnt, false_cnt, true_cnt / (true_cnt + false_cnt))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de43f3610355f051a4a7d1ec68e5cd39983800d0bb5000cb4a591287222bab46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
