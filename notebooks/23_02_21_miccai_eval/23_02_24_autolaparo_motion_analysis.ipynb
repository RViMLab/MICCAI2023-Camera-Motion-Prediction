{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autolaparo\n",
    "It can be seen that\n",
    "- Left / Right, zoom-in / zoom-out motions correlate a little with camera motion extraction\n",
    "- Individual sequences contain multiple labels and are not well isolated\n",
    "- Impossible to use for evaluation\n",
    "- Should poll motion sequences from heichole / cholec80 instead, using mean or similar\n",
    "\n",
    "- for out purposes, one should learn on **non-static data**, ie data where running average exceeds certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.io import load_yaml\n",
    "\n",
    "server = \"local\"\n",
    "server = load_yaml(\"../../config/servers.yml\")[server]\n",
    "\n",
    "increment = 5\n",
    "\n",
    "database = server[\"database\"][\"location\"]\n",
    "prefix = \"autolaparo_single_frames/AutoLaparo_Task2\"\n",
    "motion_pickle = f\"23_02_24_pre_processed_frame_increment_{increment}_frames_between_clips_1_log.pkl\"\n",
    "label_pickle = \"laparoscope_motion_label.csv\"\n",
    "\n",
    "# from readme\n",
    "motion_dict = {\n",
    "    0: \"Static\",\n",
    "    1: \"Up\",\n",
    "    2: \"Down\",\n",
    "    3: \"Left\",\n",
    "    4: \"Right\",\n",
    "    5: \"Zoom-in\",\n",
    "    6: \"Zoom-out\",\n",
    "}\n",
    "\n",
    "motion_df = pd.read_pickle(os.path.join(database, prefix, motion_pickle))\n",
    "motion_df = motion_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# motion_df = motion_df.sort_values(by=[\"vid\", \"frame\"]).reset_index(drop=True)\n",
    "label_df = pd.read_csv(os.path.join(database, prefix, label_pickle))\n",
    "label_df[\"Clip\"] = label_df[\"Clip\"].apply(lambda x: x - 1)\n",
    "\n",
    "print(label_df)\n",
    "print(motion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.processing import classify_duv_motion, dataframe_duv_running_average\n",
    "\n",
    "# de-couple duv\n",
    "\n",
    "df_split, mean, std = dataframe_duv_running_average(motion_df, window=20)\n",
    "\n",
    "df_split[\"classes\"] = df_split.apply(lambda x: classify_duv_motion(x[\"duv_0_0\"], x[\"duv_0_1\"], x[\"duv_1_0\"], x[\"duv_1_1\"], x[\"duv_2_0\"], x[\"duv_2_1\"], x[\"duv_3_0\"], x[\"duv_3_1\"], mean + 1.), axis=1)\n",
    "print(df_split)\n",
    "print(df_split.classes.unique())\n",
    "# print(df_split[df_split[\"classes\"] == \"left\"])\n",
    "\n",
    "# rolling average per video\n",
    "# 25 fps at 5 frame increment\n",
    "# df_split.groupby(\"vid\")\n",
    "id = 60\n",
    "print(\"autolaparo class: {}\".format(motion_dict[label_df.iloc[id][\"Label\"]]))\n",
    "df_split_id = df_split[df_split[\"vid\"] == id]\n",
    "df_split_id = df_split_id[::increment]\n",
    "# df_split_id = df_split_id.rolling(20, min_periods=1).mean()\n",
    "df_split_id.duv_0_0.plot()\n",
    "df_split_id.duv_0_1.plot()\n",
    "df_split_id.duv_1_0.plot()\n",
    "df_split_id.duv_1_1.plot()\n",
    "df_split_id.duv_2_0.plot()\n",
    "df_split_id.duv_2_1.plot()\n",
    "df_split_id.duv_3_0.plot()\n",
    "df_split_id.duv_3_1.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_df = motion_df[::increment]\n",
    "print(motion_df.shape)\n",
    "\n",
    "for key in motion_dict:\n",
    "    print(f\"Analyzing {motion_dict[key]} motion\")\n",
    "    clips = label_df[label_df[\"Label\"] == key].Clip.values\n",
    "    frames = motion_df[motion_df[\"vid\"].isin(clips)].reset_index(drop=True)\n",
    "    \n",
    "    duv0 = np.array(frames[\"duv\"].apply(lambda x: x[0]).to_list())\n",
    "    duv1 = np.array(frames[\"duv\"].apply(lambda x: x[1]).to_list())\n",
    "    duv2 = np.array(frames[\"duv\"].apply(lambda x: x[2]).to_list())\n",
    "    duv3 = np.array(frames[\"duv\"].apply(lambda x: x[3]).to_list())\n",
    "\n",
    "    print(f\"duv0: {duv0.mean(axis=0)} pm {duv0.std(axis=0)}\")\n",
    "    print(f\"duv1: {duv1.mean(axis=0)} pm {duv1.std(axis=0)}\")\n",
    "    print(f\"duv2: {duv2.mean(axis=0)} pm {duv2.std(axis=0)}\")\n",
    "    print(f\"duv3: {duv3.mean(axis=0)} pm {duv3.std(axis=0)}\")\n",
    "\n",
    "    plt.scatter(duv0[:, 0], duv0[:, 1], label=\"duv0\")\n",
    "    plt.scatter(duv1[:, 0], duv1[:, 1], label=\"duv1\")\n",
    "    plt.scatter(duv2[:, 0], duv2[:, 1], label=\"duv2\")\n",
    "    plt.scatter(duv3[:, 0], duv3[:, 1], label=\"duv3\")\n",
    "    plt.xlim(-100, 100)\n",
    "    plt.ylim(-100, 100)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion as Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 2\n",
    "\n",
    "def scalar_product(a: np.ndarray) -> np.ndarray:\n",
    "    return np.array([np.dot(a[i], a[i+1]) / (np.linalg.norm(a[i]) * np.linalg.norm(a[i+1])) for i in range(len(a) - 1)])\n",
    "\n",
    "def no_sign_change(a: np.ndarray, horizon: int=10) -> np.ndarray:\n",
    "    no_change = [True]*(len(a)-horizon)\n",
    "    for i in range(len(a) - horizon):\n",
    "        for j in range(horizon):\n",
    "            no_change[i] = no_change[i] and np.sign(a[i]) == np.sign(a[i+j])\n",
    "    return np.array(no_change)\n",
    "        \n",
    "\n",
    "def no_sign_changes(a: np.ndarray, b: np.ndarray, c: np.ndarray, d: np.ndarray) -> np.ndarray:\n",
    "    return np.array([a[i] and b[i] and c[i] and d[i] for i in range(len(a))])\n",
    "\n",
    "for key in motion_dict:\n",
    "    print(f\"Analyzing {motion_dict[key]} motion\")\n",
    "    clips = label_df[label_df[\"Label\"] == key].Clip.values\n",
    "    frames = motion_df[motion_df[\"vid\"].isin(clips)].reset_index(drop=True)\n",
    "    \n",
    "    duv0 = np.array(frames[\"duv\"].apply(lambda x: x[0]).to_list())\n",
    "    duv1 = np.array(frames[\"duv\"].apply(lambda x: x[1]).to_list())\n",
    "    duv2 = np.array(frames[\"duv\"].apply(lambda x: x[2]).to_list())\n",
    "    duv3 = np.array(frames[\"duv\"].apply(lambda x: x[3]).to_list())\n",
    "\n",
    "    # compute scalar product of previous and current motion vectors\n",
    "    duv0_no_sign_change = no_sign_change(scalar_product(duv0), horizon)\n",
    "    duv1_no_sign_change = no_sign_change(scalar_product(duv1), horizon)\n",
    "    duv2_no_sign_change = no_sign_change(scalar_product(duv2), horizon)\n",
    "    duv3_no_sign_change = no_sign_change(scalar_product(duv3), horizon)\n",
    "\n",
    "    no_changes = no_sign_changes(duv0_no_sign_change, duv1_no_sign_change, duv2_no_sign_change, duv3_no_sign_change)\n",
    "\n",
    "    # get duvs where no sign change occured\n",
    "    duv0 = duv0[:-horizon, :][np.argwhere(no_changes)].squeeze()\n",
    "    duv1 = duv1[:-horizon, :][np.argwhere(no_changes)].squeeze()\n",
    "    duv2 = duv2[:-horizon, :][np.argwhere(no_changes)].squeeze()\n",
    "    duv3 = duv3[:-horizon, :][np.argwhere(no_changes)].squeeze()\n",
    "\n",
    "    plt.scatter(duv0[:, 0], duv0[:, 1], label=\"duv0\")\n",
    "    plt.scatter(duv1[:, 0], duv1[:, 1], label=\"duv1\")\n",
    "    plt.scatter(duv2[:, 0], duv2[:, 1], label=\"duv2\")\n",
    "    plt.scatter(duv3[:, 0], duv3[:, 1], label=\"duv3\")\n",
    "    plt.xlim(-100, 100)\n",
    "    plt.ylim(-100, 100)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in motion_dict:\n",
    "    print(f\"Analyzing {motion_dict[key]} motion\")\n",
    "    clips = label_df[label_df[\"Label\"] == key].Clip.values\n",
    "    frames = motion_df[motion_df[\"vid\"].isin(clips)].reset_index(drop=True)\n",
    "    \n",
    "    duv0 = np.array(frames[\"duv\"].apply(lambda x: x[0]).to_list())\n",
    "    duv1 = np.array(frames[\"duv\"].apply(lambda x: x[1]).to_list())\n",
    "    duv2 = np.array(frames[\"duv\"].apply(lambda x: x[2]).to_list())\n",
    "    duv3 = np.array(frames[\"duv\"].apply(lambda x: x[3]).to_list())\n",
    "\n",
    "    # duv0 = duv0[np.linalg.norm(duv0, axis=1) > 40]\n",
    "    # duv1 = duv1[np.linalg.norm(duv1, axis=1) > 40]\n",
    "    # duv2 = duv2[np.linalg.norm(duv2, axis=1) > 40]\n",
    "    # duv3 = duv3[np.linalg.norm(duv3, axis=1) > 40]\n",
    "\n",
    "    duv0 = duv0.sum(axis=0)\n",
    "    duv1 = duv1.sum(axis=0)\n",
    "    duv2 = duv2.sum(axis=0)\n",
    "    duv3 = duv3.sum(axis=0)\n",
    "\n",
    "    max_norm = 0\n",
    "    norms = [\n",
    "        np.linalg.norm(duv0),\n",
    "        np.linalg.norm(duv1),\n",
    "        np.linalg.norm(duv2),\n",
    "        np.linalg.norm(duv3),\n",
    "    ]\n",
    "    for norm in norms:\n",
    "        if norm > max_norm:\n",
    "            max_norm = norm\n",
    "\n",
    "    duv0 = duv0 / max_norm\n",
    "    duv1 = duv1 / max_norm\n",
    "    duv2 = duv2 / max_norm\n",
    "    duv3 = duv3 / max_norm\n",
    "\n",
    "    plt.scatter(duv0[1], duv0[0], label=\"duv0\")\n",
    "    plt.scatter(duv1[1], duv1[0], label=\"duv1\")\n",
    "    plt.scatter(duv2[1], duv2[0], label=\"duv2\")\n",
    "    plt.scatter(duv3[1], duv3[0], label=\"duv3\")\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Video of classified Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from kornia import tensor_to_image, image_to_tensor\n",
    "from kornia.geometry import resize, warp_perspective\n",
    "import PIL\n",
    "from IPython.display import Image, display, clear_output, TextDisplayObject\n",
    "\n",
    "from utils import four_point_homography_to_matrix, image_edges\n",
    "from utils.viz import create_blend_from_four_point_homography\n",
    "\n",
    "shape = [240, 320]\n",
    "display_shape = [480, 640]\n",
    "motion_id = 3\n",
    "\n",
    "# motion_dict = {\n",
    "#     0: \"Static\",\n",
    "#     1: \"Up\",\n",
    "#     2: \"Down\",\n",
    "#     3: \"Left\",\n",
    "#     4: \"Right\",\n",
    "#     5: \"Zoom-in\",\n",
    "#     6: \"Zoom-out\",\n",
    "# }\n",
    "\n",
    "# unique_motion_df = motion_df[motion_df[\"vid\"].isin(label_df[label_df[\"Label\"] == motion_id].Clip.values)].reset_index(drop=True)\n",
    "unique_motion_df = df_split[df_split[\"vid\"].isin(label_df[label_df[\"Label\"] == motion_id].Clip.values)].reset_index(drop=True)\n",
    "\n",
    "# write a video using opencv\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(f'/tmp/output_{motion_id}.avi',fourcc, 10.0, (1280, 480))\n",
    "\n",
    "cnt = 0\n",
    "for idx, row in unique_motion_df.iterrows():\n",
    "    img = np.load(os.path.join(database, prefix, row.folder, row.file))\n",
    "    wrp = np.load(os.path.join(database, prefix, unique_motion_df.iloc[idx + increment].folder, unique_motion_df.iloc[idx + increment].file))\n",
    "    img, wrp = resize(image_to_tensor(img, keepdim=False).float()/255., shape), resize(image_to_tensor(wrp, keepdim=False).float()/255., shape)\n",
    "\n",
    "    # duv = torch.tensor(row.duv).unsqueeze(0)\n",
    "    duv = torch.tensor([\n",
    "        [row.duv_0_0, row.duv_0_1],\n",
    "        [row.duv_1_0, row.duv_1_1],\n",
    "        [row.duv_2_0, row.duv_2_1],\n",
    "        [row.duv_3_0, row.duv_3_1],\n",
    "    ]).unsqueeze(0)\n",
    "\n",
    "    # h = four_point_homography_to_matrix(image_edges(img), -duv)\n",
    "    # wrp_est = warp_perspective(img, h, dsize=img.shape[-2:])\n",
    "    # wrp = (tensor_to_image(wrp, keepdim=False)*255.).astype(np.uint8)\n",
    "    # wrp_est = (tensor_to_image(wrp_est, keepdim=False)*255.).astype(np.uint8)\n",
    "    # concat = np.concatenate([wrp, wrp_est], axis=1)\n",
    "    \n",
    "    blend = create_blend_from_four_point_homography(wrp, img, -duv)\n",
    "    blend = resize(blend, display_shape)\n",
    "    img = resize(img, display_shape)\n",
    "    blend = (tensor_to_image(blend, keepdim=False)*255.).astype(np.uint8)\n",
    "    img = (tensor_to_image(img, keepdim=False)*255.).astype(np.uint8)\n",
    "    concat = np.concatenate([blend, img[...,::-1]], axis=1)\n",
    "\n",
    "    # display(PIL.Image.fromarray(concat))\n",
    "    # display(TextDisplayObject(\"AutoLaparo label: {}, refined label: {}\".format(motion_dict[label_df.iloc[row.vid][\"Label\"]], row.classes)).data)\n",
    "    # clear_output(wait=True)\n",
    "\n",
    "    text = \"AutoLaparo label: {}, our re-label: {}\".format(motion_dict[label_df.iloc[row.vid][\"Label\"]], row.classes)\n",
    "    concat = cv2.putText(concat.copy(), text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    out.write(concat)\n",
    "    if cnt >1000:\n",
    "        break\n",
    "    cnt+=1\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch110",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de43f3610355f051a4a7d1ec68e5cd39983800d0bb5000cb4a591287222bab46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
