{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Hand Labelled Homography\n",
    "Generate a pipeline for quantitative analysis of homography estimation. Do\n",
    " - Randomly sample sequences from list of videos\n",
    " - Safe sequences into folder\n",
    " - Annotate some sequences\n",
    " - Create evaluation pipeline, precision, drift\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    database train                                       file  \\\n",
       "0   cholec80  True  {'name': 'video01.mp4', 'path': 'videos'}   \n",
       "1   cholec80  True  {'name': 'video02.mp4', 'path': 'videos'}   \n",
       "2   cholec80  True  {'name': 'video03.mp4', 'path': 'videos'}   \n",
       "3   cholec80  True  {'name': 'video04.mp4', 'path': 'videos'}   \n",
       "4   cholec80  True  {'name': 'video05.mp4', 'path': 'videos'}   \n",
       "..       ...   ...                                        ...   \n",
       "70  cholec80  True  {'name': 'video17.mp4', 'path': 'videos'}   \n",
       "71  cholec80  True  {'name': 'video18.mp4', 'path': 'videos'}   \n",
       "72  cholec80  True  {'name': 'video19.mp4', 'path': 'videos'}   \n",
       "73  cholec80  True  {'name': 'video20.mp4', 'path': 'videos'}   \n",
       "74  cholec80  True  {'name': 'video22.mp4', 'path': 'videos'}   \n",
       "\n",
       "                                       pre_transforms  \\\n",
       "0   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "1   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "2   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "3   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "4   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "..                                                ...   \n",
       "70  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "71  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "72  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "73  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "74  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "\n",
       "                                       aug_transforms auxiliary  \n",
       "0   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "1   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "2   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "3   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "4   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "..                                                ...       ...  \n",
       "70  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "71  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "72  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "73  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "74  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "\n",
       "[75 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>database</th>\n      <th>train</th>\n      <th>file</th>\n      <th>pre_transforms</th>\n      <th>aug_transforms</th>\n      <th>auxiliary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video01.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video02.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video03.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video04.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video05.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video17.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video18.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video19.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video20.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video22.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n  </tbody>\n</table>\n<p>75 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from dotmap import DotMap\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.io import load_yaml\n",
    "\n",
    "server = 'local'\n",
    "servers = load_yaml('../config/servers.yml')\n",
    "server = DotMap(servers[server])\n",
    "\n",
    "meta_df = pd.read_pickle('../config/cholec80_transforms.pkl')\n",
    "meta_df"
   ]
  },
  {
   "source": [
    "## Randomly Sample Image Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vid_idx: 14, frame_idx: 48924\n",
      "vid_idx: 9, frame_idx: 43711\n",
      "vid_idx: 0, frame_idx: 24431\n",
      "vid_idx: 21, frame_idx: 1726\n",
      "vid_idx: 18, frame_idx: 20768\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from kornia import tensor_to_image\n",
    "\n",
    "from utils.transforms import anyDictListToCompose\n",
    "from utils.sampling import RandomSequences\n",
    "from utils.io import generate_path\n",
    "\n",
    "debug = False\n",
    "\n",
    "max_seq = 5\n",
    "paths = meta_df.apply(lambda x: os.path.join(server.database.location, x.database, x.file['path'], x.file['name']), axis=1).tolist()\n",
    "seq_len = 100\n",
    "strides = [1]\n",
    "\n",
    "# append to tensor transform, as meta_df is supposed to operate on tensors\n",
    "to_tensor = {'module': 'torchvision.transforms', 'type': 'ToTensor', 'kwargs': {}}\n",
    "transforms = meta_df.apply(lambda x: anyDictListToCompose([to_tensor] + x.pre_transforms), axis=1).tolist()\n",
    "\n",
    "random_sequences = RandomSequences(\n",
    "    max_seq=max_seq,\n",
    "    paths=paths,\n",
    "    seq_len=seq_len,\n",
    "    transforms=transforms,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "out_prefix = 'out/homography_labelling'\n",
    "\n",
    "for seq, vid_idx, frame_idx in random_sequences:\n",
    "    print('vid_idx: {}, frame_idx: {}'.format(vid_idx, frame_idx))\n",
    "    for idx, frame in enumerate(seq):\n",
    "        frame = (tensor_to_image(frame)*255).astype(np.uint8)\n",
    "        if debug:\n",
    "            cv2.imshow('random_frame', frame)  # show images\n",
    "            cv2.waitKey()\n",
    "        else:\n",
    "            vid_path = os.path.join(out_prefix, 'vid_{}_frame_{}'.format(vid_idx. frame_idx))\n",
    "            generate_path(vid_path)\n",
    "            cv2.imwrite(os.path.join(vid_path, 'frame_{}.png'.format(frame_idx + idx*strides[0])), frame)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "### Dataset on Sampled Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     folder             file  vid  frame\n",
       "0     vid_0  frame_24431.png    0  24431\n",
       "1     vid_0  frame_24432.png    0  24432\n",
       "2     vid_0  frame_24433.png    0  24433\n",
       "3     vid_0  frame_24434.png    0  24434\n",
       "4     vid_0  frame_24435.png    0  24435\n",
       "..      ...              ...  ...    ...\n",
       "495  vid_21   frame_1821.png   21   1821\n",
       "496  vid_21   frame_1822.png   21   1822\n",
       "497  vid_21   frame_1823.png   21   1823\n",
       "498  vid_21   frame_1824.png   21   1824\n",
       "499  vid_21   frame_1825.png   21   1825\n",
       "\n",
       "[500 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>folder</th>\n      <th>file</th>\n      <th>vid</th>\n      <th>frame</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>vid_0</td>\n      <td>frame_24431.png</td>\n      <td>0</td>\n      <td>24431</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>vid_0</td>\n      <td>frame_24432.png</td>\n      <td>0</td>\n      <td>24432</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>vid_0</td>\n      <td>frame_24433.png</td>\n      <td>0</td>\n      <td>24433</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>vid_0</td>\n      <td>frame_24434.png</td>\n      <td>0</td>\n      <td>24434</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>vid_0</td>\n      <td>frame_24435.png</td>\n      <td>0</td>\n      <td>24435</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>vid_21</td>\n      <td>frame_1821.png</td>\n      <td>21</td>\n      <td>1821</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>vid_21</td>\n      <td>frame_1822.png</td>\n      <td>21</td>\n      <td>1822</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>vid_21</td>\n      <td>frame_1823.png</td>\n      <td>21</td>\n      <td>1823</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>vid_21</td>\n      <td>frame_1824.png</td>\n      <td>21</td>\n      <td>1824</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>vid_21</td>\n      <td>frame_1825.png</td>\n      <td>21</td>\n      <td>1825</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from utils.io import recursive_scan2df\n",
    "\n",
    "prefix = 'out/homography_labelling'\n",
    "\n",
    "# create a simplified dataframe\n",
    "postfix = '.png'\n",
    "df = recursive_scan2df(prefix, postfix)\n",
    "df['vid'] = df.folder.apply(lambda x: int(x.split('_')[-1]))\n",
    "df['frame'] = df.file.apply(lambda x: int(x.split('_')[-1].replace(postfix, '')))\n",
    "df = df.sort_values(['vid', 'frame']).reset_index(drop=True)\n",
    "\n",
    "out_path = 'light_log'\n",
    "df.to_pickle(os.path.join(prefix, out_path + '.pkl'))\n",
    "\n",
    "df"
   ]
  },
  {
   "source": [
    "## Estimate Homographies on Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Forward-Backward Consistency"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running with CUDA backend.\n",
      "tensor(-1.2412, device='cuda:0')\n",
      "tensor(1.1628, device='cuda:0')\n",
      "tensor(-0.0392, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from kornia import tensor_to_image, warp_perspective\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.viz import yt_alpha_blend\n",
    "from utils.transforms import anyDictListToCompose\n",
    "from utils.processing import forward_backward_sequence, image_edges, four_point_homography_to_matrix\n",
    "from datasets import ImageSequenceDataset\n",
    "from lightning_modules import DeepImageHomographyEstimationModuleBackbone\n",
    "\n",
    "def visualize(fw_img, fw_wrp, bw_img, bw_wrp, fw_duv, bw_duv):\n",
    "    fw_uv, bw_uv = image_edges(fw_img), image_edges(bw_img)\n",
    "    fw_H, bw_H = four_point_homography_to_matrix(fw_uv, fw_duv), four_point_homography_to_matrix(bw_uv, bw_duv)\n",
    "\n",
    "    fw_pred_wrp = warp_perspective(fw_img, torch.inverse(fw_H), fw_img.shape[-2:])\n",
    "    bw_pred_wrp = warp_perspective(bw_img, torch.inverse(bw_H), bw_img.shape[-2:])\n",
    "    \n",
    "    fw_blend, bw_blend = yt_alpha_blend(fw_wrp, fw_pred_wrp), yt_alpha_blend(bw_wrp, bw_pred_wrp)\n",
    "    fw_blend, bw_blend = tensor_to_image(fw_blend), tensor_to_image(bw_blend)\n",
    "\n",
    "    cv2.imshow('fw_blend', fw_blend[0])\n",
    "    cv2.imshow('bw_blend', bw_blend[0])\n",
    "    cv2.waitKey()\n",
    "\n",
    "prefix = 'out/homography_labelling'\n",
    "df = pd.read_pickle(os.path.join(prefix, 'light_log.pkl'))\n",
    "seq_len = 48\n",
    "\n",
    "transforms = [\n",
    "    {'module': 'torchvision.transforms', 'type': 'ConvertImageDtype', 'kwargs': {'dtype': torch.float}}\n",
    "]\n",
    "transforms = [anyDictListToCompose(transforms) for _ in range(len(meta_df))]\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df=df,\n",
    "    prefix=prefix,\n",
    "    seq_len=seq_len,\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "# load network\n",
    "prefix = '/home/martin/Tresors/homography_imitation_learning_logs/deep_image_homography_estimation_backbone/version_2'\n",
    "configs = load_yaml(os.path.join(prefix, 'configs.yml'))\n",
    "model = DeepImageHomographyEstimationModuleBackbone.load_from_checkpoint(os.path.join(prefix, 'checkpoints/epoch=49.ckpt'), shape=configs['model']['shape'])\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('Running with CUDA backend.')\n",
    "    device = 'cuda'\n",
    "\n",
    "model.to(device)\n",
    "model = model.eval()\n",
    "model.freeze()\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "\n",
    "dl = DataLoader(ds, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "for batch in dl:\n",
    "    fw, bw = forward_backward_sequence(batch)\n",
    "\n",
    "    # create pairs\n",
    "    fw_img, fw_wrp = fw[:,:-1:].view((-1,) + fw.shape[-3:]).to(device), fw[:,1::].view((-1,) + fw.shape[-3:]).to(device) \n",
    "    bw_img, bw_wrp = bw[:,:-1:].view((-1,) + bw.shape[-3:]).to(device), bw[:,1::].view((-1,) + bw.shape[-3:]).to(device)\n",
    "    \n",
    "    fw_duv = model(fw_img, fw_wrp)\n",
    "    bw_duv = model(bw_img, bw_wrp)\n",
    "\n",
    "    # compute error\n",
    "    print(fw_duv.mean(axis=0).mean())\n",
    "    print(bw_duv.mean(axis=0).mean())\n",
    "    duv = torch.cat((fw_duv, bw_duv))\n",
    "    print(duv.mean(axis=0).mean())\n",
    "    break\n",
    "\n",
    "    # visualize(fw_img, fw_wrp, bw_img, bw_wrp, fw_duv, bw_duv)\n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "    # for fw_frame, bw_frame in zip(fw[0], bw[0]):\n",
    "    #     fw_frame, bw_frame = tensor_to_image(fw_frame), tensor_to_image(bw_frame)\n",
    "\n",
    "    #     cv2.imshow('fw', fw_frame)\n",
    "    #     cv2.imshow('bw', bw_frame)\n",
    "    #     cv2.waitKey()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}