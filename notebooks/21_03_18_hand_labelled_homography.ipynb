{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Hand Labelled Homography\n",
    "Generate a pipeline for quantitative analysis of homography estimation. Do\n",
    " - Randomly sample sequences from list of videos\n",
    " - Safe sequences into folder\n",
    " - Annotate some sequences\n",
    " - Create evaluation pipeline, precision, drift\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    database train                                       file  \\\n",
       "0   cholec80  True  {'name': 'video01.mp4', 'path': 'videos'}   \n",
       "1   cholec80  True  {'name': 'video02.mp4', 'path': 'videos'}   \n",
       "2   cholec80  True  {'name': 'video03.mp4', 'path': 'videos'}   \n",
       "3   cholec80  True  {'name': 'video04.mp4', 'path': 'videos'}   \n",
       "4   cholec80  True  {'name': 'video05.mp4', 'path': 'videos'}   \n",
       "..       ...   ...                                        ...   \n",
       "70  cholec80  True  {'name': 'video17.mp4', 'path': 'videos'}   \n",
       "71  cholec80  True  {'name': 'video18.mp4', 'path': 'videos'}   \n",
       "72  cholec80  True  {'name': 'video19.mp4', 'path': 'videos'}   \n",
       "73  cholec80  True  {'name': 'video20.mp4', 'path': 'videos'}   \n",
       "74  cholec80  True  {'name': 'video22.mp4', 'path': 'videos'}   \n",
       "\n",
       "                                       pre_transforms  \\\n",
       "0   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "1   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "2   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "3   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "4   [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "..                                                ...   \n",
       "70  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "71  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "72  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "73  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "74  [{'module': 'utils.transforms', 'type': 'Crop'...   \n",
       "\n",
       "                                       aug_transforms auxiliary  \n",
       "0   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "1   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "2   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "3   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "4   [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "..                                                ...       ...  \n",
       "70  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "71  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "72  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "73  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "74  [{'module': 'torchvision.transforms', 'type': ...        {}  \n",
       "\n",
       "[75 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>database</th>\n      <th>train</th>\n      <th>file</th>\n      <th>pre_transforms</th>\n      <th>aug_transforms</th>\n      <th>auxiliary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video01.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video02.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video03.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video04.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video05.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video17.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video18.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video19.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video20.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>cholec80</td>\n      <td>True</td>\n      <td>{'name': 'video22.mp4', 'path': 'videos'}</td>\n      <td>[{'module': 'utils.transforms', 'type': 'Crop'...</td>\n      <td>[{'module': 'torchvision.transforms', 'type': ...</td>\n      <td>{}</td>\n    </tr>\n  </tbody>\n</table>\n<p>75 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from dotmap import DotMap\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.io import load_yaml\n",
    "\n",
    "server = 'local'\n",
    "servers = load_yaml('../config/servers.yml')\n",
    "server = DotMap(servers[server])\n",
    "\n",
    "meta_df = pd.read_pickle('../config/cholec80_transforms.pkl')\n",
    "meta_df"
   ]
  },
  {
   "source": [
    "## Randomly Sample Image Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vid_idx: 60, frame_idx: 10966\n",
      "vid_idx: 58, frame_idx: 10791\n",
      "vid_idx: 6, frame_idx: 53189\n",
      "vid_idx: 36, frame_idx: 14944\n",
      "vid_idx: 14, frame_idx: 15351\n",
      "vid_idx: 22, frame_idx: 27941\n",
      "vid_idx: 56, frame_idx: 68257\n",
      "vid_idx: 30, frame_idx: 8978\n",
      "vid_idx: 8, frame_idx: 10455\n",
      "vid_idx: 0, frame_idx: 5980\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from kornia import tensor_to_image\n",
    "\n",
    "from utils.transforms import anyDictListToCompose\n",
    "from utils.sampling import RandomSequences\n",
    "from utils.io import generate_path\n",
    "\n",
    "debug = False\n",
    "\n",
    "max_seq = 10\n",
    "paths = meta_df.apply(lambda x: os.path.join(server.database.location, x.database, x.file['path'], x.file['name']), axis=1).tolist()\n",
    "seq_len = 20\n",
    "strides = [5]\n",
    "\n",
    "# append to tensor transform, as meta_df is supposed to operate on tensors\n",
    "to_tensor = {'module': 'torchvision.transforms', 'type': 'ToTensor', 'kwargs': {}}\n",
    "transforms = meta_df.apply(lambda x: anyDictListToCompose([to_tensor] + x.pre_transforms), axis=1).tolist()\n",
    "\n",
    "random_sequences = RandomSequences(\n",
    "    max_seq=max_seq,\n",
    "    paths=paths,\n",
    "    seq_len=seq_len,\n",
    "    strides=strides,\n",
    "    transforms=transforms,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "out_prefix = 'out/homography_labelling'\n",
    "\n",
    "for seq, vid_idx, frame_idx in random_sequences:\n",
    "    print('vid_idx: {}, frame_idx: {}'.format(vid_idx, frame_idx))\n",
    "    for idx, frame in enumerate(seq):\n",
    "        frame = (tensor_to_image(frame)*255).astype(np.uint8)\n",
    "        if debug:\n",
    "            cv2.imshow('random_frame', frame)  # show images\n",
    "            cv2.waitKey()\n",
    "        else:\n",
    "            vid_path = os.path.join(out_prefix, 'vid_{}_frame_{}'.format(vid_idx, frame_idx))\n",
    "            generate_path(vid_path)\n",
    "            cv2.imwrite(os.path.join(vid_path, 'frame_{}.png'.format(frame_idx + idx*strides[0])), frame)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "### Dataset on Sampled Frames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 folder             file  vid  frame\n",
       "0      vid_0_frame_5980   frame_5980.png    0   5980\n",
       "1      vid_0_frame_5980   frame_5985.png    0   5985\n",
       "2      vid_0_frame_5980   frame_5990.png    0   5990\n",
       "3      vid_0_frame_5980   frame_5995.png    0   5995\n",
       "4      vid_0_frame_5980   frame_6000.png    0   6000\n",
       "..                  ...              ...  ...    ...\n",
       "175  vid_60_frame_10966  frame_11041.png   60  11041\n",
       "176  vid_60_frame_10966  frame_11046.png   60  11046\n",
       "177  vid_60_frame_10966  frame_11051.png   60  11051\n",
       "178  vid_60_frame_10966  frame_11056.png   60  11056\n",
       "179  vid_60_frame_10966  frame_11061.png   60  11061\n",
       "\n",
       "[180 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>folder</th>\n      <th>file</th>\n      <th>vid</th>\n      <th>frame</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>vid_0_frame_5980</td>\n      <td>frame_5980.png</td>\n      <td>0</td>\n      <td>5980</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>vid_0_frame_5980</td>\n      <td>frame_5985.png</td>\n      <td>0</td>\n      <td>5985</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>vid_0_frame_5980</td>\n      <td>frame_5990.png</td>\n      <td>0</td>\n      <td>5990</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>vid_0_frame_5980</td>\n      <td>frame_5995.png</td>\n      <td>0</td>\n      <td>5995</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>vid_0_frame_5980</td>\n      <td>frame_6000.png</td>\n      <td>0</td>\n      <td>6000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>vid_60_frame_10966</td>\n      <td>frame_11041.png</td>\n      <td>60</td>\n      <td>11041</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>vid_60_frame_10966</td>\n      <td>frame_11046.png</td>\n      <td>60</td>\n      <td>11046</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>vid_60_frame_10966</td>\n      <td>frame_11051.png</td>\n      <td>60</td>\n      <td>11051</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>vid_60_frame_10966</td>\n      <td>frame_11056.png</td>\n      <td>60</td>\n      <td>11056</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>vid_60_frame_10966</td>\n      <td>frame_11061.png</td>\n      <td>60</td>\n      <td>11061</td>\n    </tr>\n  </tbody>\n</table>\n<p>180 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from utils.io import recursive_scan2df\n",
    "\n",
    "prefix = 'out/homography_labelling'\n",
    "\n",
    "# create a simplified dataframe\n",
    "postfix = '.png'\n",
    "df = recursive_scan2df(prefix, postfix)\n",
    "df['vid'] = df.folder.apply(lambda x: int(x.split('_')[-3]))\n",
    "df['frame'] = df.file.apply(lambda x: int(x.split('_')[-1].replace(postfix, '')))\n",
    "df = df.sort_values(['vid', 'frame']).reset_index(drop=True)\n",
    "\n",
    "out_path = 'light_log'\n",
    "df.to_pickle(os.path.join(prefix, out_path + '.pkl'))\n",
    "\n",
    "df"
   ]
  },
  {
   "source": [
    "## Estimate Homographies on Sequences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hand Labelled"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.io import recursive_scan2df\n",
    "from utils.viz import yt_alpha_blend\n",
    "\n",
    "prefix = 'out/homography_labelling'\n",
    "vid_df = pd.read_pickle(os.path.join(prefix, 'light_log.pkl'))\n",
    "pts_df = recursive_scan2df(prefix, '.txt')\n",
    "\n",
    "for _, row in pts_df.iterrows():\n",
    "    df = pd.read_csv(os.path.join(prefix, row.folder, row.file), delimiter='\\t', header=None, names=['x', 'y']) # stored as x, y opencv convention\n",
    "    n_img = len(vid_df[vid_df.folder == row.folder])\n",
    "    n_pts = int(len(df) / n_img)\n",
    "\n",
    "    print('N Points: {}'.format(n_pts))\n",
    "    \n",
    "    # get homography\n",
    "    for img_idx in range(n_img - 1):\n",
    "        src_pts = df.iloc[img_idx*n_pts:(img_idx+1)*n_pts]\n",
    "        dst_pts = df.iloc[(img_idx+1)*n_pts:(img_idx+2)*n_pts]\n",
    "        \n",
    "        src_pts = np.array([[pt.x, pt.y] for _, pt in src_pts.iterrows()])\n",
    "        dst_pts = np.array([[pt.x, pt.y] for _, pt in dst_pts.iterrows()])\n",
    "\n",
    "        H, _ = cv2.findHomography(src_pts, dst_pts)\n",
    "        \n",
    "        # show images\n",
    "        img = cv2.imread(os.path.join(prefix, vid_df[vid_df.folder == row.folder].iloc[img_idx].folder, vid_df[vid_df.folder == row.folder].iloc[img_idx].file))\n",
    "        wrp = cv2.imread(os.path.join(prefix, vid_df[vid_df.folder == row.folder].iloc[img_idx + 1].folder, vid_df[vid_df.folder == row.folder].iloc[img_idx + 1].file))\n",
    "        cv2.imshow('img', img)\n",
    "        cv2.imshow('wrp', wrp)\n",
    "\n",
    "        # create blend\n",
    "        wrp_pred = cv2.warpPerspective(img, H, (img.shape[1], img.shape[0]))\n",
    "        cv2.imshow('wrp_pred', wrp_pred)\n",
    "        blend = yt_alpha_blend(wrp, wrp_pred)\n",
    "        cv2.imshow('blend', blend/255)  # note that more points are needed, or blend will only work locally around pts\n",
    "        cv2.waitKey()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "### Forward-Backward Consistency"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running with CUDA backend.\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n",
      "torch.Size([1, 2, 3, 480, 640]) torch.Size([1, 2, 3, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.functional as F\n",
    "from kornia import tensor_to_image, warp_perspective\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.viz import yt_alpha_blend\n",
    "from utils.transforms import anyDictListToCompose\n",
    "from utils.processing import forward_backward_sequence, image_edges, four_point_homography_to_matrix\n",
    "from datasets import ImageSequenceDataset\n",
    "from lightning_modules import DeepImageHomographyEstimationModuleBackbone\n",
    "\n",
    "def visualize(fw_img, fw_wrp, bw_img, bw_wrp, fw_duv, bw_duv):\n",
    "    fw_uv, bw_uv = image_edges(fw_img), image_edges(bw_img)\n",
    "    fw_H, bw_H = four_point_homography_to_matrix(fw_uv, fw_duv), four_point_homography_to_matrix(bw_uv, bw_duv)\n",
    "\n",
    "    fw_pred_wrp = warp_perspective(fw_img, torch.inverse(fw_H), fw_img.shape[-2:])\n",
    "    bw_pred_wrp = warp_perspective(bw_img, torch.inverse(bw_H), bw_img.shape[-2:])\n",
    "    \n",
    "    fw_blend, bw_blend = yt_alpha_blend(fw_wrp, fw_pred_wrp), yt_alpha_blend(bw_wrp, bw_pred_wrp)\n",
    "    fw_blend, bw_blend = tensor_to_image(fw_blend), tensor_to_image(bw_blend)\n",
    "\n",
    "    cv2.imshow('fw_blend', fw_blend)\n",
    "    cv2.imshow('bw_blend', bw_blend)\n",
    "    cv2.waitKey()\n",
    "\n",
    "prefix = 'out/homography_labelling'\n",
    "# prefix = '/media/martin/Samsung_T5/data/endoscopic_data/camera_motion_separated_png/without_camera_motion'\n",
    "df = pd.read_pickle(os.path.join(prefix, 'light_log.pkl'))\n",
    "# df = pd.read_pickle(os.path.join(prefix, 'light_log_without_camera_motion.pkl'))\n",
    "seq_len = 2\n",
    "\n",
    "transforms = [\n",
    "    {'module': 'torchvision.transforms', 'type': 'ConvertImageDtype', 'kwargs': {'dtype': torch.float}}\n",
    "]\n",
    "transforms = [anyDictListToCompose(transforms) for _ in range(len(meta_df))]\n",
    "\n",
    "ds = ImageSequenceDataset(\n",
    "    df=df,\n",
    "    prefix=prefix,\n",
    "    seq_len=seq_len,\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "# load network\n",
    "prefix = '/home/martin/Tresors/homography_imitation_learning_logs/deep_image_homography_estimation_backbone/version_2'\n",
    "configs = load_yaml(os.path.join(prefix, 'configs.yml'))\n",
    "model = DeepImageHomographyEstimationModuleBackbone.load_from_checkpoint(os.path.join(prefix, 'checkpoints/epoch=49.ckpt'), shape=configs['model']['shape'])\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('Running with CUDA backend.')\n",
    "    device = 'cuda'\n",
    "\n",
    "model.to(device)\n",
    "model = model.eval()\n",
    "model.freeze()\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "\n",
    "dl = DataLoader(ds, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "fw_mean_distance = []\n",
    "bw_mean_distance = []\n",
    "con_mean_distance = []\n",
    "\n",
    "I = 100\n",
    "for idx, batch in enumerate(dl):\n",
    "    fw, bw = forward_backward_sequence(batch)\n",
    "    print(fw.shape, bw.shape)\n",
    "   \n",
    "\n",
    "    # create pairs\n",
    "    fw_img, fw_wrp = fw[:,:-1:].view((-1,) + fw.shape[-3:]).to(device), fw[:,1::].view((-1,) + fw.shape[-3:]).to(device) \n",
    "    bw_img, bw_wrp = bw[:,:-1:].view((-1,) + bw.shape[-3:]).to(device), bw[:,1::].view((-1,) + bw.shape[-3:]).to(device)\n",
    "\n",
    "    # fw_img, fw_wrp, bw_img, bw_wrp = tensor_to_image(fw_img[0]), tensor_to_image(fw_wrp[0]), tensor_to_image(bw_img[0]), tensor_to_image(bw_wrp[0])\n",
    "    # cv2.imshow('fw_img', fw_img)\n",
    "    # cv2.imshow('fw_wrp', fw_wrp)\n",
    "    # cv2.imshow('bw_img', bw_img)\n",
    "    # cv2.imshow('bw_wrp', bw_wrp)\n",
    "    # cv2.waitKey()\n",
    "\n",
    "    fw_duv = model(fw_img, fw_wrp)\n",
    "    bw_duv = model(bw_img, bw_wrp)\n",
    "\n",
    "    # compute error, sum isnt that great of a measurement as 2*N vs N elements\n",
    "    fw_mean_distance.append(torch.linalg.norm(fw_duv.sum(axis=0), axis=1).mean().item())\n",
    "    bw_mean_distance.append(torch.linalg.norm(bw_duv.sum(axis=0), axis=1).mean().item())\n",
    "    duv = torch.cat((fw_duv, bw_duv))\n",
    "    con_mean_distance.append(torch.linalg.norm(duv.sum(axis=0), axis=1).mean().item())\n",
    "\n",
    "    if idx == I - 1:\n",
    "        break\n",
    "\n",
    "    visualize(fw_img, fw_wrp, bw_img, bw_wrp, fw_duv, bw_duv)\n",
    "\n",
    "fw_mean_distance = np.array(fw_mean_distance)\n",
    "bw_mean_distance = np.array(bw_mean_distance)\n",
    "con_mean_distance = np.array(con_mean_distance)\n",
    "\n",
    "print(fw_mean_distance.mean(), fw_mean_distance.var())\n",
    "print(bw_mean_distance.mean(), bw_mean_distance.var())\n",
    "print(con_mean_distance.mean(), con_mean_distance.var())\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}